{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1 -- Comprehend Lab\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of SageMaker BlazingText to perform supervised binary/multi class with single or multi label text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DEPENDENCIES\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import re\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::969219788367:role/service-role/AmazonSageMaker-ExecutionRole-20181112T160917\n",
      "The S3 bucket you are using to upload your dataset is: rga-aws-ai-workshop-pod-9999\n"
     ]
    }
   ],
   "source": [
    "#Prepare for the lab by learning the current IAM Role and the S3 bucket we will use...\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "# REPLACE THE POD BELOW WITH THE NUMBER ASSIGN TO YOU \n",
    "pod = '9999'\n",
    "region = 'us-east-2'\n",
    "prefix = 'comprehend' #we will store the dataset we will use to train a custom classification classifier\n",
    "\n",
    "bucket = \"rga-aws-ai-workshop-pod-\" + pod # Replace with your own bucket name if needed\n",
    "print(\"The S3 bucket you are using to upload your dataset is:\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.\n",
      "\n",
      "{\n",
      "    \"Sentiment\": \"POSITIVE\",\n",
      "    \"SentimentScore\": {\n",
      "        \"Positive\": 0.999503493309021,\n",
      "        \"Negative\": 6.865667819511145e-05,\n",
      "        \"Neutral\": 0.0004137569048907608,\n",
      "        \"Mixed\": 1.4093851859797724e-05\n",
      "    },\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"d553d6a2-c55d-4d9d-a2bf-4588e13f0551\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"x-amzn-requestid\": \"d553d6a2-c55d-4d9d-a2bf-4588e13f0551\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"content-length\": \"165\",\n",
      "            \"date\": \"Tue, 15 Oct 2019 17:19:38 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run the first review through Comprehend native service.\n",
    "client = boto3.client('comprehend')\n",
    "\n",
    "#Lets first run the first Airbnb review through the native sentiment analysis of Comprehend to see whats available\n",
    "#without customization\n",
    "textreview = 'We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.'\n",
    "print(textreview + '\\n')\n",
    "\n",
    "response = client.detect_sentiment(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(json.dumps(response, indent=4))\n",
    "\n",
    "#Note the ability to extract the overall sentiment from the reviews. Comprehend can provide: positive, negative,\n",
    "#neutral, and mixed from the text with no training/tuning\n",
    "#This can also be run from the Comprehend AWS Console interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.\n",
      "{\n",
      "    \"Entities\": [\n",
      "        {\n",
      "            \"Score\": 0.9984429478645325,\n",
      "            \"Type\": \"LOCATION\",\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 11,\n",
      "            \"EndOffset\": 18\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.46189820766448975,\n",
      "            \"Type\": \"PERSON\",\n",
      "            \"Text\": \"Hamilton\",\n",
      "            \"BeginOffset\": 26,\n",
      "            \"EndOffset\": 34\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9971076846122742,\n",
      "            \"Type\": \"PERSON\",\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 311,\n",
      "            \"EndOffset\": 314\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9949488639831543,\n",
      "            \"Type\": \"LOCATION\",\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 454,\n",
      "            \"EndOffset\": 461\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9917650818824768,\n",
      "            \"Type\": \"PERSON\",\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 492,\n",
      "            \"EndOffset\": 495\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"99c196a1-b3c7-4c40-9970-29495abf699c\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"x-amzn-requestid\": \"99c196a1-b3c7-4c40-9970-29495abf699c\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"content-length\": \"488\",\n",
      "            \"date\": \"Tue, 15 Oct 2019 17:20:44 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n",
      "\n",
      "We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.\n",
      "{\n",
      "    \"KeyPhrases\": [\n",
      "        {\n",
      "            \"Score\": 0.9986284971237183,\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 11,\n",
      "            \"EndOffset\": 18\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.5389164090156555,\n",
      "            \"Text\": \"Hamilton\",\n",
      "            \"BeginOffset\": 26,\n",
      "            \"EndOffset\": 34\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9982622265815735,\n",
      "            \"Text\": \"our teenaged daughter\",\n",
      "            \"BeginOffset\": 53,\n",
      "            \"EndOffset\": 74\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9282022714614868,\n",
      "            \"Text\": \"this location and neighborhood\",\n",
      "            \"BeginOffset\": 85,\n",
      "            \"EndOffset\": 115\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9946176409721375,\n",
      "            \"Text\": \"tons\",\n",
      "            \"BeginOffset\": 145,\n",
      "            \"EndOffset\": 149\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9767205715179443,\n",
      "            \"Text\": \"nearby places\",\n",
      "            \"BeginOffset\": 153,\n",
      "            \"EndOffset\": 166\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9925803542137146,\n",
      "            \"Text\": \"groceries\",\n",
      "            \"BeginOffset\": 181,\n",
      "            \"EndOffset\": 190\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9858511686325073,\n",
      "            \"Text\": \"The condo\",\n",
      "            \"BeginOffset\": 192,\n",
      "            \"EndOffset\": 201\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9911547899246216,\n",
      "            \"Text\": \"necessities\",\n",
      "            \"BeginOffset\": 255,\n",
      "            \"EndOffset\": 266\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9931622743606567,\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 311,\n",
      "            \"EndOffset\": 314\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9977700710296631,\n",
      "            \"Text\": \"a great host\",\n",
      "            \"BeginOffset\": 319,\n",
      "            \"EndOffset\": 331\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9979057312011719,\n",
      "            \"Text\": \"our questions\",\n",
      "            \"BeginOffset\": 359,\n",
      "            \"EndOffset\": 372\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9788888692855835,\n",
      "            \"Text\": \"a plus\",\n",
      "            \"BeginOffset\": 381,\n",
      "            \"EndOffset\": 387\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.8212679028511047,\n",
      "            \"Text\": \"parking\",\n",
      "            \"BeginOffset\": 396,\n",
      "            \"EndOffset\": 403\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9994938373565674,\n",
      "            \"Text\": \"our car\",\n",
      "            \"BeginOffset\": 432,\n",
      "            \"EndOffset\": 439\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9744483828544617,\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 454,\n",
      "            \"EndOffset\": 461\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9939947724342346,\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 492,\n",
      "            \"EndOffset\": 495\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"84ff9f76-a493-4136-83fe-af28988c7be4\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"x-amzn-requestid\": \"84ff9f76-a493-4136-83fe-af28988c7be4\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"content-length\": \"1420\",\n",
      "            \"date\": \"Tue, 15 Oct 2019 17:20:44 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Lets also use Comprehend to detect entities and key phrases for our Airbnb review...\n",
    "response = client.detect_entities(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(textreview)\n",
    "print(json.dumps(response, indent=4) + '\\n')\n",
    "\n",
    "\n",
    "response = client.detect_key_phrases(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(textreview)\n",
    "print(json.dumps(response, indent=4))\n",
    "\n",
    "#Make note of the different information that can be extracted from the review with no ML model training or tuning....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions to Update the IAM Role for the Sagemaker Notebook\n",
    "Need to provide access to:\n",
    "\n",
    "- The S3 bucket we will create\n",
    "- Comprehend\n",
    "- Sagemaker stuff\n",
    "- Create a new IAM role for Comprehend DataAccessRoleArn & include the S3 bucket that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClientError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ba48f26fb1dd>\u001b[0m in \u001b[0;36mcreate_bucket\u001b[0;34m(bucket_name, region)\u001b[0m\n\u001b[1;32m     21\u001b[0m             s3_client.create_bucket(Bucket=bucket_name,\n\u001b[0;32m---> 22\u001b[0;31m                                     CreateBucketConfiguration=location)\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m: An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ba48f26fb1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcreate_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-ba48f26fb1dd>\u001b[0m in \u001b[0;36mcreate_bucket\u001b[0;34m(bucket_name, region)\u001b[0m\n\u001b[1;32m     21\u001b[0m             s3_client.create_bucket(Bucket=bucket_name,\n\u001b[1;32m     22\u001b[0m                                     CreateBucketConfiguration=location)\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClientError' is not defined"
     ]
    }
   ],
   "source": [
    "#CREATE THE BUCKET to be used for the dataset\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "create_bucket(bucket, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airbnb-reviews-training.csv\n",
      "﻿notgreat,We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again\n",
      "notgreat,\"Chris' place is really lovely! Plenty of space for 2 people, with lots of thoughtful touches like biscuits, bottled water, tea and coffee etc! The kitchen was well equipped to make meals, and Chris quickly provided a can opener when we asked to borrow one. The washer and dryer in the apartment were also very handy. Good location in a residential neighbourhood, about 10-15 min walk to the local Aldi and 10 mins to shops, including the currency exchange to get bus tickets. 5 mins walk from the bus stop that takes you to the blue line, which then takes you to O'hare airport or downtown. It takes around an hour to get to the Museum campus, and about 45 mins to attractions like the Skydeck and Hancock Tower. Chris recommended an architecture boat tour which was fab. Communication was always fast and helpful, would definitely recommend!\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the dataset\n",
    "!ls comprehend/\n",
    "!head comprehend/airbnb-reviews-training.csv -n 2\n",
    "\n",
    "#Note that the data has two columns, the first is a custom label for 'great' and 'notgreat' to indicate\n",
    "#the custom labels that have been applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UPLOAD THE DATASET TO S3 FROM THE LOCAL SYSTEM FOR COMPREHEND CUSTOM TRAINING\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "upload_file('comprehend/airbnb-reviews-training.csv', bucket)\n",
    "upload_file('comprehend/airbnb-reviews-holdout.csv', bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create IAM Role for Comprehend to read S3 data from your bucket\n",
    "\n",
    "Create Policy:\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"s3:GetObject\",\n",
    "            \"s3:ListBucket\",\n",
    "            \"s3:PutObject\"\n",
    "        ],\n",
    "        \"Resource\": [\n",
    "            \"arn:aws:s3:::rga-aws-ai-workshop-pod-9999\",\n",
    "            \"arn:aws:s3:::rga-aws-ai-workshop-pod-9999\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "Create Role:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create response:\n",
      "{'DocumentClassifierArn': 'arn:aws:comprehend:us-east-2:969219788367:document-classifier/RGAAirbnb-9999', 'ResponseMetadata': {'RequestId': '5b7dce71-8f02-4d85-a1a1-766ef15683e7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '5b7dce71-8f02-4d85-a1a1-766ef15683e7', 'content-type': 'application/x-amz-json-1.1', 'content-length': '104', 'date': 'Tue, 15 Oct 2019 19:09:38 GMT'}, 'RetryAttempts': 0}}\n",
      "Describe response:\n",
      "{'DocumentClassifierProperties': {'DocumentClassifierArn': 'arn:aws:comprehend:us-east-2:969219788367:document-classifier/RGAAirbnb-9999', 'LanguageCode': 'en', 'Status': 'SUBMITTED', 'SubmitTime': datetime.datetime(2019, 10, 15, 19, 9, 39, 78000, tzinfo=tzlocal()), 'InputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9999/comprehend/airbnb-reviews-training.csv'}, 'OutputDataConfig': {}, 'DataAccessRoleArn': 'arn:aws:iam::969219788367:role/comprehend-rga'}, 'ResponseMetadata': {'RequestId': 'aeae082b-3970-412d-ae8b-a706348d663e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'aeae082b-3970-412d-ae8b-a706348d663e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '422', 'date': 'Tue, 15 Oct 2019 19:09:38 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Boto3 SDK:\n",
    "client = boto3.client('comprehend', region_name=region)\n",
    "s3_uri = 's3://' + bucket + '/' + prefix + '/' + 'airbnb-reviews-training.csv'\n",
    "\n",
    "#UPDATE THE ARN FROM THE ROLE YOU CREATED IN THE PREVIOUS SECTION\n",
    "dataaccessarn='arn:aws:iam::969219788367:role/comprehend-rga'\n",
    "\n",
    "docclassifier='RGAAirbnb-' + pod\n",
    "\n",
    "# Create a document classifier\n",
    "create_response = client.create_document_classifier(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_uri\n",
    "    },\n",
    "    DataAccessRoleArn=dataaccessarn,\n",
    "    DocumentClassifierName=docclassifier,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print('Create response:\\n' + str(create_response))\n",
    "\n",
    "# Check the status of the classifier\n",
    "docclassifierarn = create_response['DocumentClassifierArn']\n",
    "describe_response = client.describe_document_classifier(\n",
    "    DocumentClassifierArn=docclassifierarn)\n",
    "print('Describe response:\\n' + str(describe_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe response:\n",
      "{'DocumentClassifierProperties': {'DocumentClassifierArn': 'arn:aws:comprehend:us-east-2:969219788367:document-classifier/RGAAirbnb-9999', 'LanguageCode': 'en', 'Status': 'TRAINED', 'SubmitTime': datetime.datetime(2019, 10, 15, 19, 9, 39, 78000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2019, 10, 15, 19, 22, 19, 874000, tzinfo=tzlocal()), 'TrainingStartTime': datetime.datetime(2019, 10, 15, 19, 12, 5, 663000, tzinfo=tzlocal()), 'TrainingEndTime': datetime.datetime(2019, 10, 15, 19, 20, 47, 490000, tzinfo=tzlocal()), 'InputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9999/comprehend/airbnb-reviews-training.csv'}, 'OutputDataConfig': {}, 'ClassifierMetadata': {'NumberOfLabels': 2, 'NumberOfTrainedDocuments': 1152, 'NumberOfTestDocuments': 127, 'EvaluationMetrics': {'Accuracy': 0.9055, 'Precision': 0.4528, 'Recall': 0.5, 'F1Score': 0.4752}}, 'DataAccessRoleArn': 'arn:aws:iam::969219788367:role/comprehend-rga'}, 'ResponseMetadata': {'RequestId': '66f65791-92ff-4ecf-941c-4301c083c67b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '66f65791-92ff-4ecf-941c-4301c083c67b', 'content-type': 'application/x-amz-json-1.1', 'content-length': '797', 'date': 'Tue, 15 Oct 2019 20:24:47 GMT'}, 'RetryAttempts': 0}}\n",
      "\n",
      "\n",
      "Custom Model Accuracy:\n",
      "{'Accuracy': 0.9055, 'Precision': 0.4528, 'Recall': 0.5, 'F1Score': 0.4752}\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the classifier\n",
    "# Look for a status on TRAINED before moving to the next step\n",
    "describe_response = client.describe_document_classifier(\n",
    "    DocumentClassifierArn=docclassifierarn)\n",
    "print('Describe response:\\n' + str(describe_response))\n",
    "\n",
    "if describe_response['DocumentClassifierProperties']['Status'] == 'TRAINED':\n",
    "    print('\\n\\nCustom Model Accuracy:\\n' + str(describe_response['DocumentClassifierProperties']['ClassifierMetadata']['EvaluationMetrics']))\n",
    "\n",
    "# Alternatively please review the Comprehend AWS Console GUI to validate if your custom job is still training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start response:\n",
      " {'JobId': 'f2bcf157993c25ad18f34e08227ec7ed', 'JobStatus': 'SUBMITTED', 'ResponseMetadata': {'RequestId': '626922fa-84dc-47f2-8176-750325487cb0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '626922fa-84dc-47f2-8176-750325487cb0', 'content-type': 'application/x-amz-json-1.1', 'content-length': '68', 'date': 'Tue, 15 Oct 2019 19:58:02 GMT'}, 'RetryAttempts': 0}}\n",
      "Describe response:\n",
      " {'DocumentClassificationJobProperties': {'JobId': 'f2bcf157993c25ad18f34e08227ec7ed', 'JobName': 'RGAAirbnb-Job-9999', 'JobStatus': 'SUBMITTED', 'SubmitTime': datetime.datetime(2019, 10, 15, 19, 58, 2, 195000, tzinfo=tzlocal()), 'DocumentClassifierArn': 'arn:aws:comprehend:us-east-2:969219788367:document-classifier/RGAAirbnb-9999', 'InputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9999/comprehend/airbnb-reviews-holdout.csv', 'InputFormat': 'ONE_DOC_PER_LINE'}, 'OutputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9999/comprehend/output/969219788367-CLN-f2bcf157993c25ad18f34e08227ec7ed/output/output.tar.gz'}, 'DataAccessRoleArn': 'arn:aws:iam::969219788367:role/comprehend-rga'}, 'ResponseMetadata': {'RequestId': 'c049f207-9f0b-47a1-ab3c-fdf3a7601a3a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c049f207-9f0b-47a1-ab3c-fdf3a7601a3a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '629', 'date': 'Tue, 15 Oct 2019 19:58:02 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "client = boto3.client('comprehend', region_name=region)\n",
    "s3_uri_in = 's3://' + bucket + '/' + prefix + '/' + 'airbnb-reviews-holdout.csv'\n",
    "s3_uri_out = 's3://' + bucket + '/' + prefix + '/' + 'output'\n",
    "jobname='RGAAirbnb-Job-' + pod\n",
    "\n",
    "start_response = client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_uri_in,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_uri_out\n",
    "    },\n",
    "    DataAccessRoleArn=dataaccessarn,\n",
    "    DocumentClassifierArn=docclassifierarn,\n",
    "    JobName=jobname\n",
    ")\n",
    "\n",
    "print(\"Start response:\\n\", start_response)\n",
    "\n",
    "# Check the status of the job\n",
    "describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "print(\"Describe response:\\n\", describe_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe response:\n",
      " {'DocumentClassificationJobProperties': {'JobId': 'f2bcf157993c25ad18f34e08227ec7ed', 'JobName': 'RGAAirbnb-Job-9999', 'JobStatus': 'COMPLETED', 'SubmitTime': datetime.datetime(2019, 10, 15, 19, 58, 2, 195000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2019, 10, 15, 20, 2, 38, 293000, tzinfo=tzlocal()), 'DocumentClassifierArn': 'arn:aws:comprehend:us-east-2:969219788367:document-classifier/RGAAirbnb-9999', 'InputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9999/comprehend/airbnb-reviews-holdout.csv', 'InputFormat': 'ONE_DOC_PER_LINE'}, 'OutputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9999/comprehend/output/969219788367-CLN-f2bcf157993c25ad18f34e08227ec7ed/output/output.tar.gz'}, 'DataAccessRoleArn': 'arn:aws:iam::969219788367:role/comprehend-rga'}, 'ResponseMetadata': {'RequestId': 'fecd57e7-e40f-409f-9e32-ea39a75279da', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'fecd57e7-e40f-409f-9e32-ea39a75279da', 'content-type': 'application/x-amz-json-1.1', 'content-length': '656', 'date': 'Tue, 15 Oct 2019 20:03:30 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the job.\n",
    "# When JobStatus is COMPLETED you can move to the next step.\n",
    "describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "print(\"Describe response:\\n\", describe_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://rga-aws-ai-workshop-pod-9999/comprehend/output/969219788367-CLN-f2bcf157993c25ad18f34e08227ec7ed/output/output.tar.gz to comprehend/output.tar.gz\n",
      "predictions.jsonl\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"0\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8829}, {\"Name\": \"great\", \"Score\": 0.1172}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"1\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8356}, {\"Name\": \"great\", \"Score\": 0.1644}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"2\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9084}, {\"Name\": \"great\", \"Score\": 0.0917}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"3\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8934}, {\"Name\": \"great\", \"Score\": 0.1066}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"4\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8968}, {\"Name\": \"great\", \"Score\": 0.1032}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"5\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8526}, {\"Name\": \"great\", \"Score\": 0.1474}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"6\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8697}, {\"Name\": \"great\", \"Score\": 0.1304}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"7\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9105}, {\"Name\": \"great\", \"Score\": 0.0895}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"8\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8945}, {\"Name\": \"great\", \"Score\": 0.1055}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"9\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8874}, {\"Name\": \"great\", \"Score\": 0.1126}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"10\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8602}, {\"Name\": \"great\", \"Score\": 0.1398}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"11\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9237}, {\"Name\": \"great\", \"Score\": 0.0764}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"12\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8333}, {\"Name\": \"great\", \"Score\": 0.1667}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"13\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.905}, {\"Name\": \"great\", \"Score\": 0.095}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"14\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8689}, {\"Name\": \"great\", \"Score\": 0.1311}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"15\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8146}, {\"Name\": \"great\", \"Score\": 0.1854}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"16\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8689}, {\"Name\": \"great\", \"Score\": 0.1311}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"17\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9173}, {\"Name\": \"great\", \"Score\": 0.0827}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"18\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8149}, {\"Name\": \"great\", \"Score\": 0.1852}]}\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"19\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.889}, {\"Name\": \"great\", \"Score\": 0.111}]}\n"
     ]
    }
   ],
   "source": [
    "output_s3 = describe_response['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri']\n",
    "#print(output_s3)\n",
    "!aws s3 cp {output_s3} comprehend/output.tar.gz\n",
    "!tar -xvzf comprehend/output.tar.gz -C comprehend\n",
    "!head comprehend/predictions.jsonl -n 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2 -- Sagemaker model training & delivery using a native algorithm (BlazingText)\n",
    "\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of SageMaker BlazingText to perform supervised binary/multi class with single or multi label text classification. BlazingText can train the model on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. BlazingText extends the fastText text classifier to leverage GPU acceleration using custom CUDA kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::969219788367:role/service-role/AmazonSageMaker-ExecutionRole-20181112T160917\n",
      "sagemaker-us-east-2-969219788367\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import re\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = \"sagemaker-us-east-2-969219788367\" # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'blazingtext/supervised' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Now we'll download a dataset from the web on which we want to train the text classification model. BlazingText expects a single preprocessed text file with space separated tokens and each line of the file should contain a single sentence and the corresponding label(s) prefixed by \"\\__label\\__\".\n",
    "\n",
    "In this example, let us train the text classification model on the [DBPedia Ontology Dataset](https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014#2) as done by [Zhang et al](https://arxiv.org/pdf/1509.01626.pdf). The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014. It has 560,000 training samples and 70,000 testing samples. The fields we used for this dataset contain title and abstract of each Wikipedia article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 125968\r\n",
      "drwxrwxrwx 4 ec2-user ec2-user     4096 Sep 26 17:03 .\r\n",
      "drwxr-xr-x 5 ec2-user ec2-user     4096 Sep 26 15:35 ..\r\n",
      "-rw-rw-rw- 1 ec2-user ec2-user    38604 Sep 26 16:47 blazingtext_text_classification_dbpedia.ipynb\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user    38428 Sep 26 17:03 blazingtext_text_classification_rga_airbnb.ipynb\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user       18 Sep 26 17:03 classes.txt\r\n",
      "drwxrwxr-x 2 ec2-user ec2-user     4096 Mar 29  2015 dbpedia_csv\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 68431223 Sep 26 15:37 dbpedia_csv.tar.gz\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 36675292 Sep 26 16:29 dbpedia.train\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 22950227 Sep 26 16:29 dbpedia.validation\r\n",
      "drwxrwxr-x 2 ec2-user ec2-user     4096 Sep 26 16:47 .ipynb_checkpoints\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user    22881 Sep 26 17:03 test.csv\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   799616 Sep 26 17:03 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "#!wget https://github.com/saurabh3949/Text-Classification-Datasets/raw/master/dbpedia_csv.tar.gz\n",
    "\n",
    "labels_file = \"classes.txt\"\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file(labels_file, labels_file)\n",
    "s3.Bucket(bucket).download_file(train_file, train_file)\n",
    "s3.Bucket(bucket).download_file(test_file, test_file)\n",
    "!ls -la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘dbpedia_csv’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "#!tar -xzvf dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the dataset and the classes to get some understanding about how the data and the label is provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿0,\"Explore Logan Square from Renovated Condo with Free Parking\",\"The apartment was lovely and the beds were very comfortable. I was traveling with 3 younger girls (12, 11, and 11) and I loved that the location was near plenty of coffee shops, restaurants, and parks. The area is definitely a mix of lower to mid income residents and students. With plenty of funky, high end, hipster spots. We felt very safe walking the neighborhood. And we had plenty of room in the space to spread out! The biggest plus was the free parking. Leo and Alex were gracious enough to allow us to use the lot a bit early, before check in, and a bit later, after check out. All things were accessible by Uber in 5 minutes or so. ~20 Uber ride to downtown. I would definitely give this spot another visit and recommend it to others.\"\r",
      "\r\n",
      "0,\"Explore Logan Square from Renovated Condo with Free Parking\",\"Our family enjoyed the space. The check-in process was easy. The photos and description are accurate. The house was very clean. It’s very nice that it has a private parking space. The parking is at the back and there is a narrow pathway next to the house where you can walk from the parking lot to the front door. This can be a bit inconvenient if you are carrying many luggage. We love the kitchen and dinning area where we cooked breakfast and heated up leftovers. The only thing is the air condition was quite loud. But we don’t get too much noise in the bedrooms.\"\r",
      "\r\n",
      "0,\"Foodie Paradise in Logan Square area - Sunny 1 BDR\",\"This one bedroom apartment is an Air BnBer’s dream. We have stayed in Air BnBs around the world and this is among the best. - Clearly, the home’s owner thought through everything that a guest might need. We arrived to find coffee and tea waiting, which was perfect after a long day of travel. - The apartment is clean, spacious, well located and very, very comfortable. It is the best Ikea styling I have ever seen in real life. In fact, Ikea would do well to stage a photo shoot here. The furniture, the decoration, the artwork are all simple, yet we felt like we could move right in and live here as a forever home. - •\tThe bed is like a cozy nest (loved the pillows). - •\tThe kitchen (and the fridge in particular) were sparkling clean. It was stocked with more than the basics, which was great since we always shop for breakfast food when we arrive in a new city, and found a Trader Joe’s quite nearby. - •\tThere is a little office with many kinds of connectors for phones and computers - •\tThe living room couch is really comfortable, and there’s a lap blanket for cuddling up. - •\tThe bathroom was outfitted with much more than we needed. The towels were soft and fluffy and there were plenty of them. - •\tThe lights and heat are controlled by a Nest system with sensors to turn lights come on and off as you enter and leave a room. That no doubt helps with the energy use but it takes a little human adjustment We ended up taping over the bathroom light sensor so it didn’t blast on in the middle of the night when one of us wandered into the bathroom. - •\tThe building is on a busy main street, which concerned us as we arrived -- I am very sensitive to noise. But once inside, the apartment is completely quiet. If the weather had been warm enough to open windows, it might have been noisier, but we didn’t test it out. - •\tStreet parking is a little competitive and cars have to be moved off the street in the later afternoon when the parking lane becomes a traffic lane. This was not an issue for us since we were gone pretty much all day long. - Everything was as it was presented by Natalia, who was a thoughtful and attentive hostess. We were in town to help our daughter move into an apartment walking distance from the Air BnB. We hope to stay here again on return visits.\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#!head dbpedia_csv/train.csv -n 3\n",
    "!head train.csv -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above output, the CSV has 3 fields - Label index, title and abstract. Let us first create a label index to label name mapping and then proceed to preprocess the dataset for ingestion by BlazingText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will print the labels file (`classes.txt`) to see all possible labels followed by creating an index to label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿NotGreat\r",
      "\r\n",
      "Great"
     ]
    }
   ],
   "source": [
    "#!cat dbpedia_csv/classes.txt\n",
    "!cat classes.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the mapping from integer indices to class label which will later be used to retrieve the actual class name during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'NotGreat', '1': 'Great'}\n"
     ]
    }
   ],
   "source": [
    "#index_to_label = {} \n",
    "#with open(\"dbpedia_csv/classes.txt\") as f:\n",
    "#    for i,label in enumerate(f.readlines()):\n",
    "#        index_to_label[str(i+1)] = label.strip()\n",
    "#print(index_to_label)\n",
    "\n",
    "index_to_label = {} \n",
    "with open(\"classes.txt\", mode='r', encoding='utf-8-sig') as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i)] = label.strip()\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to preprocess the training data into **space separated tokenized text** format which can be consumed by `BlazingText` algorithm. Also, as mentioned previously, the class label(s) should be prefixed with `__label__` and it should be present in the same line along with the original sentence. We'll use `nltk` library to tokenize the input sentences from DBPedia dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the nltk tokenizer and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, mode='r', encoding='utf-8-sig') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.5 ms, sys: 34.4 ms, total: 107 ms\n",
      "Wall time: 807 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess('train.csv', 'airbnb.train', keep=1)\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('test.csv', 'airbnb.validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 ms, sys: 9.56 ms, total: 58.4 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='airbnb.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='airbnb.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 825641698319.dkr.ecr.us-east-2.amazonaws.com/blazingtext:latest (us-east-2)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for supervised text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the original implementation of [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf), SageMaker BlazingText provides an efficient implementation of the continuous bag-of-words (CBOW) and skip-gram architectures using Negative Sampling, on CPUs and additionally on GPU[s]. The GPU implementation uses highly optimized CUDA kernels. To learn more, please refer to [*BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs*](https://dl.acm.org/citation.cfm?doid=3146347.3146354).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides skip-gram and CBOW, SageMaker BlazingText also supports the \"Batch Skipgram\" mode, which uses efficient mini-batching and matrix-matrix operations ([BLAS Level 3 routines](https://software.intel.com/en-us/mkl-developer-reference-fortran-blas-level-3-routines)). This mode enables distributed word2vec training across multiple CPU nodes, allowing almost linear scale up of word2vec computation to process hundreds of millions of words per second. Please refer to [*Parallelizing Word2Vec in Shared and Distributed Memory*](https://arxiv.org/pdf/1604.04661.pdf) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingText also supports a *supervised* mode for text classification. It extends the FastText text classifier to leverage GPU acceleration using custom CUDA kernels. The model can be trained on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. For more information, please refer to the [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the following modes are supported by BlazingText on different types instances:\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|                \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|      \t|          \t|        ✔       \t|     | |\n",
    "\n",
    "Now, let's define the SageMaker `Estimator` with resource configurations and hyperparameters to train Text Classification on *DBPedia* dataset, using \"supervised\" mode on a `c4.4xlarge` instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyper-parameters for this object and we have our data channels linked with the algorithm. The only  remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the `Estimator` classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take some time, depending on the size of the data. Therefore it might be a few minutes before we start getting training logs for our training jobs. The data logs will also print out Accuracy on the validation data for every epoch after training job has executed `min_epochs`. This metric is a proxy for the quality of the algorithm. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-26 17:21:48 Starting - Starting the training job...\n",
      "2019-09-26 17:21:50 Starting - Launching requested ML instances...\n",
      "2019-09-26 17:22:45 Starting - Preparing the instances for training......\n",
      "2019-09-26 17:23:44 Downloading - Downloading input data\n",
      "2019-09-26 17:23:44 Training - Downloading the training image..\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[09/26/2019 17:23:58 WARNING 139821395216192] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[09/26/2019 17:23:58 WARNING 139821395216192] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[09/26/2019 17:23:58 INFO 139821395216192] nvidia-smi took: 0.0252521038055 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[09/26/2019 17:23:58 INFO 139821395216192] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[31m[09/26/2019 17:23:58 INFO 139821395216192] Processing /opt/ml/input/data/train/airbnb.train . File size: 0 MB\u001b[0m\n",
      "\u001b[31m[09/26/2019 17:23:58 INFO 139821395216192] Processing /opt/ml/input/data/validation/airbnb.validation . File size: 0 MB\u001b[0m\n",
      "\u001b[31mRead 0M words\u001b[0m\n",
      "\u001b[31mNumber of words:  3574\u001b[0m\n",
      "\u001b[31mLoading validation data from /opt/ml/input/data/validation/airbnb.validation\u001b[0m\n",
      "\u001b[31mLoaded validation data.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[31m##### Alpha: -0.0001  Progress: 100.20%  Million Words/sec: 13.74 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 9.03 #####\n",
      "\u001b[0m\n",
      "\u001b[31mTraining finished.\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 9.03\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 0.18\n",
      "\u001b[0m\n",
      "\u001b[31m#train_accuracy: 0.8999\u001b[0m\n",
      "\u001b[31mNumber of train examples: 1269\n",
      "\u001b[0m\n",
      "\u001b[31m#validation_accuracy: 0.6667\u001b[0m\n",
      "\u001b[31mNumber of validation examples: 30\u001b[0m\n",
      "\n",
      "2019-09-26 17:24:19 Uploading - Uploading generated training model\n",
      "2019-09-26 17:24:19 Completed - Training job completed\n",
      "Training seconds: 53\n",
      "Billable seconds: 53\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON format for inference\n",
    "BlazingText supports `application/json` as the content-type for inference. The payload should contain a list of sentences with the key as \"**instances**\" while being passed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8895270228385925\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__NotGreat\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#!head test.csv -n 3\n",
    "\n",
    "all_rows = []\n",
    "with open('test.csv', mode='r', encoding='utf-8-sig') as csvinfile:\n",
    "    csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        all_rows.append(row)\n",
    "#all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "#print(all_rows)\n",
    "#pool = Pool(processes=multiprocessing.cpu_count())\n",
    "#transformed_rows = pool.map(transform_instance, all_rows)\n",
    "#pool.close() \n",
    "#pool.join()\n",
    "\n",
    "#print(all_rows[0][0])\n",
    "#print(all_rows[0][2])\n",
    "\n",
    "\n",
    "\n",
    "#sentences = [\"Convair was an american aircraft manufacturing company which later expanded into rockets and spacecraft.\",\n",
    "#            \"Berwick secondary college is situated in the outer melbourne metropolitan suburb of berwick .\"]\n",
    "\n",
    "sentences = [all_rows[29][2]]\n",
    "#print(sentences)\n",
    "print(all_rows[29][0])\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the model will return only one prediction, the one with the highest probability. For retrieving the top k predictions, you can set `k` in the configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.9971234798431396,\n",
      "      0.0017487191362306476\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Company\",\n",
      "      \"__label__MeanOfTransportation\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.9984437823295593,\n",
      "      0.0005028279265388846\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__EducationalInstitution\",\n",
      "      \"__label__OfficeHolder\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "payload = {\"instances\" : tokenized_sentences,\n",
    "          \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop / Close the Endpoint (Optional)\n",
    "Finally, we should delete the endpoint before we close the notebook if we don't need to keep the endpoint running for serving realtime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(text_classifier.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
