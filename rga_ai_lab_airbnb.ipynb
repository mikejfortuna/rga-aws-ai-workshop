{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1 -- Comprehend Lab\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of SageMaker BlazingText to perform supervised binary/multi class with single or multi label text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DEPENDENCIES\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::683164714817:role/service-role/AmazonSageMaker-ExecutionRole-20191016T110583\n",
      "The S3 bucket you will be using to upload your dataset is: rga-aws-ai-workshop-pod-9996\n"
     ]
    }
   ],
   "source": [
    "#Prepare for the lab by learning the current IAM Role and the S3 bucket we will use...\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "# REPLACE THE POD BELOW WITH THE NUMBER ASSIGN TO YOU \n",
    "pod = '9996'\n",
    "\n",
    "# REPLACE THE REGION BELOW WITH THE REGION YOU ARE OPERATING IN (ie N. Virginia=us-east-1, Ohio=us-east-2)\n",
    "region = 'us-east-1'\n",
    "\n",
    "prefix = 'comprehend' #we will store the dataset we will use to train a custom classification classifier\n",
    "\n",
    "bucket = \"rga-aws-ai-workshop-pod-\" + pod # Replace with your own bucket name if needed\n",
    "print(\"The S3 bucket you will be using to upload your dataset is:\", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions to Update the IAM Role for the Sagemaker Notebook\n",
    "Need to provide access to:\n",
    "\n",
    "- The S3 bucket we will create\n",
    "- Comprehend\n",
    "- Sagemaker stuff\n",
    "- Create a new IAM role for Comprehend DataAccessRoleArn & include the S3 bucket that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Raw text Airbnb review:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Comprehend native sentiment analysis:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Sentiment\": \"POSITIVE\",\n",
      "    \"SentimentScore\": {\n",
      "        \"Positive\": 0.999503493309021,\n",
      "        \"Negative\": 6.865667819511145e-05,\n",
      "        \"Neutral\": 0.0004137569048907608,\n",
      "        \"Mixed\": 1.4093851859797724e-05\n",
      "    },\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"e058c109-64ed-4b88-aebc-44864017599c\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"x-amzn-requestid\": \"e058c109-64ed-4b88-aebc-44864017599c\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"content-length\": \"165\",\n",
      "            \"date\": \"Thu, 17 Oct 2019 14:29:17 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run the first Airbnb review through Comprehend native service.\n",
    "client = boto3.client('comprehend')\n",
    "\n",
    "#Lets first run the first Airbnb review through the native sentiment analysis of Comprehend to see whats available\n",
    "#without customization. Please note this is a simple API call with no ML training required.\n",
    "#Feel free to re-run this cell with another review you would like to extract information on.\n",
    "textreview = 'We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.'\n",
    "printmd(\"**Raw text Airbnb review:**\")\n",
    "print(textreview + '\\n')\n",
    "\n",
    "response = client.detect_sentiment(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "printmd(\"**Comprehend native sentiment analysis:**\")\n",
    "print(json.dumps(response, indent=4))\n",
    "\n",
    "#Note the ability to extract the overall sentiment from the reviews. Comprehend can provide: positive, negative,\n",
    "#neutral, and mixed from the text with no training/tuning\n",
    "#This can also be run from the Comprehend AWS Console interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Raw text Airbnb review:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Comprehend entities detected:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Entities\": [\n",
      "        {\n",
      "            \"Score\": 0.9984429478645325,\n",
      "            \"Type\": \"LOCATION\",\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 11,\n",
      "            \"EndOffset\": 18\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.46189820766448975,\n",
      "            \"Type\": \"PERSON\",\n",
      "            \"Text\": \"Hamilton\",\n",
      "            \"BeginOffset\": 26,\n",
      "            \"EndOffset\": 34\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9971076846122742,\n",
      "            \"Type\": \"PERSON\",\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 311,\n",
      "            \"EndOffset\": 314\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9949488639831543,\n",
      "            \"Type\": \"LOCATION\",\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 454,\n",
      "            \"EndOffset\": 461\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9917650818824768,\n",
      "            \"Type\": \"PERSON\",\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 492,\n",
      "            \"EndOffset\": 495\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"57386afc-bca3-4a49-b9d7-1008c80c3672\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"x-amzn-requestid\": \"57386afc-bca3-4a49-b9d7-1008c80c3672\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"content-length\": \"488\",\n",
      "            \"date\": \"Thu, 17 Oct 2019 14:29:24 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Raw text Airbnb review:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Comprehend key phrases detected:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"KeyPhrases\": [\n",
      "        {\n",
      "            \"Score\": 0.9986284971237183,\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 11,\n",
      "            \"EndOffset\": 18\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.5389164090156555,\n",
      "            \"Text\": \"Hamilton\",\n",
      "            \"BeginOffset\": 26,\n",
      "            \"EndOffset\": 34\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9982622265815735,\n",
      "            \"Text\": \"our teenaged daughter\",\n",
      "            \"BeginOffset\": 53,\n",
      "            \"EndOffset\": 74\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9282022714614868,\n",
      "            \"Text\": \"this location and neighborhood\",\n",
      "            \"BeginOffset\": 85,\n",
      "            \"EndOffset\": 115\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9946176409721375,\n",
      "            \"Text\": \"tons\",\n",
      "            \"BeginOffset\": 145,\n",
      "            \"EndOffset\": 149\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9767205715179443,\n",
      "            \"Text\": \"nearby places\",\n",
      "            \"BeginOffset\": 153,\n",
      "            \"EndOffset\": 166\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9925803542137146,\n",
      "            \"Text\": \"groceries\",\n",
      "            \"BeginOffset\": 181,\n",
      "            \"EndOffset\": 190\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9858511686325073,\n",
      "            \"Text\": \"The condo\",\n",
      "            \"BeginOffset\": 192,\n",
      "            \"EndOffset\": 201\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9911547899246216,\n",
      "            \"Text\": \"necessities\",\n",
      "            \"BeginOffset\": 255,\n",
      "            \"EndOffset\": 266\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9931622743606567,\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 311,\n",
      "            \"EndOffset\": 314\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9977700710296631,\n",
      "            \"Text\": \"a great host\",\n",
      "            \"BeginOffset\": 319,\n",
      "            \"EndOffset\": 331\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9979057312011719,\n",
      "            \"Text\": \"our questions\",\n",
      "            \"BeginOffset\": 359,\n",
      "            \"EndOffset\": 372\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9788888692855835,\n",
      "            \"Text\": \"a plus\",\n",
      "            \"BeginOffset\": 381,\n",
      "            \"EndOffset\": 387\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.8212679028511047,\n",
      "            \"Text\": \"parking\",\n",
      "            \"BeginOffset\": 396,\n",
      "            \"EndOffset\": 403\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9994938373565674,\n",
      "            \"Text\": \"our car\",\n",
      "            \"BeginOffset\": 432,\n",
      "            \"EndOffset\": 439\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9744483828544617,\n",
      "            \"Text\": \"Chicago\",\n",
      "            \"BeginOffset\": 454,\n",
      "            \"EndOffset\": 461\n",
      "        },\n",
      "        {\n",
      "            \"Score\": 0.9939947724342346,\n",
      "            \"Text\": \"Rob\",\n",
      "            \"BeginOffset\": 492,\n",
      "            \"EndOffset\": 495\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"4d485511-3fe6-47fb-8707-7e496d303ee7\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"x-amzn-requestid\": \"4d485511-3fe6-47fb-8707-7e496d303ee7\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"content-length\": \"1420\",\n",
      "            \"date\": \"Thu, 17 Oct 2019 14:29:24 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Next lets use Comprehend to detect entities and key phrases for our Airbnb review...\n",
    "response = client.detect_entities(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "printmd(\"**Raw text Airbnb review:**\")\n",
    "print(textreview + '\\n')\n",
    "\n",
    "printmd(\"**Comprehend entities detected:**\")\n",
    "print(json.dumps(response, indent=4) + '\\n')\n",
    "\n",
    "response = client.detect_key_phrases(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "printmd(\"**Raw text Airbnb review:**\")\n",
    "print(textreview + '\\n')\n",
    "\n",
    "printmd(\"**Comprehend key phrases detected:**\")\n",
    "print(json.dumps(response, indent=4))\n",
    "\n",
    "#Make note of the different information that can be extracted from the review with no ML model training or tuning....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rga-aws-ai-workshop-pod-9996 bucket created successfully!\n"
     ]
    }
   ],
   "source": [
    "#CREATE THE BUCKET IN YOUR AWS AWS ACCT to be used for the dataset\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if (region is None) or (region == 'us-east-1'):\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return print(bucket + ' bucket created successfully!')\n",
    "\n",
    "create_bucket(bucket, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Comprehend working directory contents:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 772\n",
      "drwxrwxr-x 3 ec2-user ec2-user   4096 Oct 16 20:40 .\n",
      "drwxrwxr-x 6 ec2-user ec2-user   4096 Oct 17 14:28 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  14880 Oct 16 19:21 airbnb-reviews-holdout.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 759203 Oct 16 19:21 airbnb-reviews-training.csv\n",
      "drwxrwxr-x 2 ec2-user ec2-user   4096 Oct 16 20:00 .ipynb_checkpoints\n",
      "/home/ec2-user/SageMaker/rga-aws-ai-workshop\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**First 2 entries from the training dataset:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿notgreat,We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again\r",
      "\r\n",
      "notgreat,\"Chris' place is really lovely! Plenty of space for 2 people, with lots of thoughtful touches like biscuits, bottled water, tea and coffee etc! The kitchen was well equipped to make meals, and Chris quickly provided a can opener when we asked to borrow one. The washer and dryer in the apartment were also very handy. Good location in a residential neighbourhood, about 10-15 min walk to the local Aldi and 10 mins to shops, including the currency exchange to get bus tickets. 5 mins walk from the bus stop that takes you to the blue line, which then takes you to O'hare airport or downtown. It takes around an hour to get to the Museum campus, and about 45 mins to attractions like the Skydeck and Hancock Tower. Chris recommended an architecture boat tour which was fab. Communication was always fast and helpful, would definitely recommend!\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#Inspect the dataset. We are reviewing only the first two entries.\n",
    "printmd(\"**Comprehend working directory contents:**\")\n",
    "!ls comprehend/ -la\n",
    "!pwd\n",
    "\n",
    "printmd(\"**First 2 entries from the training dataset:**\")\n",
    "!head comprehend/airbnb-reviews-training.csv -n 2\n",
    "\n",
    "# We are assuming that the data transformations have occurred upstream and we have formated the data required for\n",
    "# Comprehend. Note that the data has two columns, the first is a custom label for 'great' or 'notgreat' to indicate\n",
    "# the custom labels that have been applied. This is the format required for the Comprehend custom classifier.\n",
    "# The first column is the custom label, the second column contains the raw review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comprehend/airbnb-reviews-training.csv object uploaded successfully!\n",
      "comprehend/airbnb-reviews-holdout.csv object uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#UPLOAD THE DATASET TO S3 FROM THE LOCAL SYSTEM FOR COMPREHEND CUSTOM TRAINING\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return print(file_name + ' object uploaded successfully!')\n",
    "\n",
    "upload_file('comprehend/airbnb-reviews-training.csv', bucket)\n",
    "upload_file('comprehend/airbnb-reviews-holdout.csv', bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create IAM Role for Comprehend to read S3 data from your bucket\n",
    "\n",
    "**Create Data Access Role for Comprehend custom classifier job:**\n",
    "\n",
    "\n",
    "**Create Policy:**\n",
    "\n",
    "`\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"s3:GetObject\",\n",
    "            \"s3:ListBucket\",\n",
    "            \"s3:PutObject\"\n",
    "        ],\n",
    "        \"Resource\": [\n",
    "            \"arn:aws:s3:::UPDATEBUCKETNAMEHERE\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rga-aws-ai-workshop-pod-9996-policy IAM policy was created\n",
      "The rga-aws-ai-workshop-pod-9996-role IAM role was created\n"
     ]
    }
   ],
   "source": [
    "# CREATE THE IAM POLICY & ROLE FOR COMPREHEND TO USE TO TRAIN THE CUSTOM MODEL\n",
    "# CREATE IAM POLICY\n",
    "policyDocumentStr = '''\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"s3:GetObject\",\n",
    "            \"s3:ListBucket\",\n",
    "            \"s3:PutObject\"\n",
    "        ],\n",
    "        \"Resource\": [\n",
    "            \"arn:aws:s3:::%s\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "'''%(bucket)\n",
    "pattern = re.compile(r'[\\s\\r\\n]+')\n",
    "policyDocumentStr = re.sub(pattern, '', policyDocumentStr)\n",
    "\n",
    "client = boto3.client('iam')\n",
    "response = client.create_policy(\n",
    "    PolicyName= bucket + '-policy',\n",
    "    PolicyDocument=policyDocumentStr,\n",
    "    Description='IAM permissions policy for Comprehend access to S3 bucket for RGA AWS AI Workshop'\n",
    ")\n",
    "print('The ' + response['Policy']['PolicyName'] + ' IAM policy was created')\n",
    "policyArn= response['Policy']['Arn']\n",
    "\n",
    "# CREATE IAM ROLE\n",
    "trustPolicyDocumentStr = '''\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"comprehend.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}'''\n",
    "pattern = re.compile(r'[\\s\\r\\n]+')\n",
    "trustPolicyDocumentStr = re.sub(pattern, '', trustPolicyDocumentStr)\n",
    "\n",
    "response = client.create_role(\n",
    "    RoleName= bucket + '-role',\n",
    "    AssumeRolePolicyDocument= trustPolicyDocumentStr,\n",
    "    Description='IAM role for Comprehend access to S3 bucket for RGA AWS AI Workshop'\n",
    ")\n",
    "print('The ' + response['Role']['RoleName'] + ' IAM role was created')\n",
    "dataaccessarn=response['Role']['Arn']\n",
    "\n",
    "# ASSIGN POLICIES TO ROLE\n",
    "response = client.attach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= policyArn\n",
    ")\n",
    "response = client.attach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= 'arn:aws:iam::aws:policy/service-role/ComprehendDataAccessRolePolicy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Create response output**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DocumentClassifierArn': 'arn:aws:comprehend:us-east-1:683164714817:document-classifier/RGAAirbnb-9996', 'ResponseMetadata': {'RequestId': '8e44b531-e573-47ec-9562-4035a517df6e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '8e44b531-e573-47ec-9562-4035a517df6e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '104', 'date': 'Thu, 17 Oct 2019 14:30:32 GMT'}, 'RetryAttempts': 0}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Describe response output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DocumentClassifierProperties': {'DocumentClassifierArn': 'arn:aws:comprehend:us-east-1:683164714817:document-classifier/RGAAirbnb-9996', 'LanguageCode': 'en', 'Status': 'SUBMITTED', 'SubmitTime': datetime.datetime(2019, 10, 17, 14, 30, 32, 534000, tzinfo=tzlocal()), 'InputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9996/comprehend/airbnb-reviews-training.csv'}, 'OutputDataConfig': {}, 'DataAccessRoleArn': 'arn:aws:iam::683164714817:role/rga-aws-ai-workshop-pod-9996-role'}, 'ResponseMetadata': {'RequestId': '4dfd15a5-d78a-42a7-874b-620325a5f5f6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '4dfd15a5-d78a-42a7-874b-620325a5f5f6', 'content-type': 'application/x-amz-json-1.1', 'content-length': '441', 'date': 'Thu, 17 Oct 2019 14:30:32 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Boto3 SDK:\n",
    "client = boto3.client('comprehend', region_name=region)\n",
    "s3_uri = 's3://' + bucket + '/' + prefix + '/' + 'airbnb-reviews-training.csv'\n",
    "\n",
    "#UPDATE THE ARN FROM THE ROLE YOU CREATED IN THE PREVIOUS SECTION\n",
    "#account_id = role.split(':')[4]\n",
    "#dataaccessarn='arn:aws:iam::12345678901234:role/rga-aws-ai-workshop-pod-xxxx'\n",
    "                    \n",
    "docclassifier='RGAAirbnb-' + pod\n",
    "\n",
    "# Create a document classifier\n",
    "create_response = client.create_document_classifier(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_uri\n",
    "    },\n",
    "    DataAccessRoleArn=dataaccessarn,\n",
    "    DocumentClassifierName=docclassifier,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "printmd(\"**Create response output**\")\n",
    "print(str(create_response) + \"\\n\")\n",
    "\n",
    "# Check the status of the classifier\n",
    "docclassifierarn = create_response['DocumentClassifierArn']\n",
    "describe_response = client.describe_document_classifier(\n",
    "    DocumentClassifierArn=docclassifierarn)\n",
    "printmd(\"**Describe response output:**\")\n",
    "print(str(describe_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Document classifier status (Wait for the status to show TRAINED):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMITTED\n",
      "SUBMITTED\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINING\n",
      "TRAINED\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Custom Model Accuracy:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.9055, 'Precision': 0.4528, 'Recall': 0.5, 'F1Score': 0.4752}\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the classifier\n",
    "# Look for a status on TRAINED before moving to the next step\n",
    "describe_response = client.describe_document_classifier(DocumentClassifierArn=docclassifierarn)\n",
    "printmd(\"**Document classifier status (Wait for the status to show TRAINED):**\")\n",
    "print(describe_response['DocumentClassifierProperties']['Status'])\n",
    "while describe_response['DocumentClassifierProperties']['Status'] != 'TRAINED':\n",
    "    time.sleep(20)\n",
    "    describe_response = client.describe_document_classifier(DocumentClassifierArn=docclassifierarn)\n",
    "    print(describe_response['DocumentClassifierProperties']['Status'])\n",
    "\n",
    "printmd(\"**Custom Model Accuracy:**\")\n",
    "print(str(describe_response['DocumentClassifierProperties']['ClassifierMetadata']['EvaluationMetrics']))\n",
    "\n",
    "# Alternatively please review the Comprehend AWS Console GUI to validate if your custom job is still training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Start response output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobId': '968b3307ae4833cf7f2f2f548b7c2457', 'JobStatus': 'SUBMITTED', 'ResponseMetadata': {'RequestId': 'b61f0f4f-498e-4e4d-bdf8-d7d7f1135131', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b61f0f4f-498e-4e4d-bdf8-d7d7f1135131', 'content-type': 'application/x-amz-json-1.1', 'content-length': '68', 'date': 'Thu, 17 Oct 2019 14:56:59 GMT'}, 'RetryAttempts': 0}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Describe response output:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DocumentClassificationJobProperties': {'JobId': '968b3307ae4833cf7f2f2f548b7c2457', 'JobName': 'RGAAirbnb-Job-9996', 'JobStatus': 'SUBMITTED', 'SubmitTime': datetime.datetime(2019, 10, 17, 14, 57, 0, 535000, tzinfo=tzlocal()), 'DocumentClassifierArn': 'arn:aws:comprehend:us-east-1:683164714817:document-classifier/RGAAirbnb-9996', 'InputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9996/comprehend/airbnb-reviews-holdout.csv', 'InputFormat': 'ONE_DOC_PER_LINE'}, 'OutputDataConfig': {'S3Uri': 's3://rga-aws-ai-workshop-pod-9996/comprehend/output/683164714817-CLN-968b3307ae4833cf7f2f2f548b7c2457/output/output.tar.gz'}, 'DataAccessRoleArn': 'arn:aws:iam::683164714817:role/rga-aws-ai-workshop-pod-9996-role'}, 'ResponseMetadata': {'RequestId': '49f2eb3a-b9a6-4c29-a9e3-aef211b19919', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '49f2eb3a-b9a6-4c29-a9e3-aef211b19919', 'content-type': 'application/x-amz-json-1.1', 'content-length': '648', 'date': 'Thu, 17 Oct 2019 14:56:59 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "client = boto3.client('comprehend', region_name=region)\n",
    "s3_uri_in = 's3://' + bucket + '/' + prefix + '/' + 'airbnb-reviews-holdout.csv'\n",
    "s3_uri_out = 's3://' + bucket + '/' + prefix + '/' + 'output'\n",
    "jobname='RGAAirbnb-Job-' + pod\n",
    "\n",
    "start_response = client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_uri_in,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_uri_out\n",
    "    },\n",
    "    DataAccessRoleArn=dataaccessarn,\n",
    "    DocumentClassifierArn=docclassifierarn,\n",
    "    JobName=jobname\n",
    ")\n",
    "\n",
    "printmd(\"**Start response output:**\")\n",
    "print(str(start_response) + '\\n')\n",
    "\n",
    "# Check the status of the job\n",
    "describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "printmd(\"**Describe response output:**\")\n",
    "print(describe_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Job training status (Wait for the status to show COMPLETED):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the job.\n",
    "# When JobStatus is COMPLETED you can move to the next step.\n",
    "describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "\n",
    "printmd(\"**Job training status (Wait for the status to show COMPLETED):**\")\n",
    "print(describe_response['DocumentClassificationJobProperties']['JobStatus'])\n",
    "\n",
    "while describe_response['DocumentClassificationJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    time.sleep(20)\n",
    "    describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "    print(describe_response['DocumentClassificationJobProperties']['JobStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Download model from S3 to local filesystem:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 433 Bytes/433 Bytes (860 Bytes/s) with 1 file(s) remaining\r",
      "download: s3://rga-aws-ai-workshop-pod-9996/comprehend/output/683164714817-CLN-968b3307ae4833cf7f2f2f548b7c2457/output/output.tar.gz to comprehend/output.tar.gz\r\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Uncompress predictions:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.jsonl\r\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Review groundtruth (first 20):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿notgreat,\"We have had an incredible trip! Rod’s apartment was perfect. His description and photos of the apartment painted an incredible picture and it still exceeded our expectations. The location couldn’t have been better as we were just a few steps from public transportation and a market for groceries was right at the corner. - - Even when we came inside to escape the rain, we had a spectacular view of the city. The whole building was super clean, comfortable, stylish, and safe. Rod was always super quick to respond to any questions we had and gave us great recommendations for local restaurants. We would love to stay here again next time we’re in Chicago.\"\r",
      "\r\n",
      "notgreat,\"Just like everyone else said, the view is spectacular. This apt is sparkling clean and breathtakingly beautiful. I was in Chicago for a work meeting and loved coming back in the evenings to this comfortable, homely, beautiful apt. I had no trouble getting in each time.. the women at the front desk were all friendly (I made a point of saying hello to each of them so they were greeting me like old friends after 2 days). It's close to everything - small market just 200 feet away allowed me to stock the fridge. I highly recommend Rod and his lovely apt.\"\r",
      "\r\n",
      "notgreat,\"Grace’s place is sparkling clean. Even on barefoot you wouldn’t feel dust on your feet. The place is so stylish and I love the simple touches on every part of her house. It is a great value for money being a 2bedroom and can also accommodate more people with the 2sofabeds in the living room. The kitchen is complete with stove, oven, microwave, fridge, cooking ware and utensils. The place is very close to grocery store, some resto and the Belmont Blue station. This is such a great place and I will definitely stay again when i go back to Chicago.\"\r",
      "\r\n",
      "notgreat,I love love love Danielle's place. It's extremely charming and well decorated. You can definitely tell she put a lot of effort into all the details of the apartment. It was perfect for a winter visit to Chicago with the amazing fireplace and heated floors. The walkways and stairs were kept clear of snow and salted. We even heard a snow blower once or twice ;). There was always parking available on the street right outside the house. Which was a major plus for us. The neighborhood was great. Plenty of places to eat and bars to check out. I would highly recommend staying here and will likely book it again.\r",
      "\r\n",
      "notgreat,\"Danielle’s space was perfect for our trip to Chicago. There were four of us, so we did two in each bed — totally comfortable. As it was February in Chicago, we loved the fireplace to keep the space nice and warm. The flat was very clean, and stocked with towels and products for the shower. Danielle gave us a mega list of recommendations ahead of time, which was super helpful and full of cool restaurants and bars. The apartment is in Logan Square, and while we spent a good deal of time there and in places like Wicker Park, the freezing weather had us in lots of Uber’s downtown as well — just something to keep in mind. Though, it’s also off a subway line. Overall, no complaints, and Danielle’s communication was amazing. Would definitely recommend.\"\r",
      "\r\n",
      "notgreat,\"Heather and JP are wonderful hosts!! Their space is incredibly stylish and really unique. We loved the garden area and enjoyed sitting outside to eat breakfast and relax. It is in an amazing location, within walking distance of so many great restaurants! We really appreciated their flexibility with check-in and all the special touches that were included. This is one of the best Airbnb's that we've ever stayed in! We'll definitely stay here again whenever we're in Chicago. If this place is open, then book it. You'll love it!\"\r",
      "\r\n",
      "notgreat,\"One of our favorite AirBnB locations ever! Hip, comfortable and in a great location. Heather and JP have thought of everything you will need to enjoy your stay. We loved the privacy of the location. It was so hip and stylish, as well as super comfy. Felt like home. You are only a 5-7 min walk to the L. Bus transport is easily accessible. As well, the neighborhood is very cool. It is close to many other fun areas of west Chicago. Not far from downtown either. We didn’t even find a need to go to the magnificent mile. There is too much fun to be had hanging out in this area of town. We did go to Logan’s Square a bit. It’s very close. Loads of good bars and food. - We would definitely stay here again. They have set a high bar for what to expect for future stays with AirBnB!!\"\r",
      "\r\n",
      "notgreat,\"We had a great stay at Heather and JP’s. Our schedule was a bit in flux but they were very accommodating to our changing arrival time. - - The space itself is really a hidden gem tucked away behind a lovely garden for privacy and quiet. The loft was so unique and charming and they did a great job to maximize the space. It’s very functional but stylish. Just a note to be careful on the steps approaching the bed/loft area and to watch your head as it’s a bit of a low clearance. - - We had an easy time street parking our car directly outside the unit which was extremely convenient and one less thing to have to worry about. We also appreciated the easy self check-in/out process. - - The location was convenient to exploring some really cool shops, bars, and restaurants nearby. It was a great way to experience some local flavor outside of the more touristy stuff. - - Thanks for a great visit!\"\r",
      "\r\n",
      "notgreat,\"My sister and I absolutely loved staying at Heather and JP's place. This was our first time booking an Airbnb and the bar is set really high for any future bookings anywhere else. The location is great. The loft is beautifully decorated, very cozy and very clean. The complimentary snacks were also a very nice touch. Our friends that were staying at a different Airbnb stopped by for a few minutes and they were upset they didnt book this first. This place is truly charming. If we're ever in Chicago we hope we can stay here again. Thanks Heather & JP for being such great hosts and providing everything one would need for a perfect stay.\"\r",
      "\r\n",
      "notgreat,\"This was the cleanest Airbnb we’ve ever stayed in. If you are picky with finding a clean place that actually FEELS clean, this home is for you. You can immediately tell that Rob takes pride in his apartment and everything is so well maintained and stylishly decorated. The bed was a welcomed place for rest after a long day, and we loved how firm the mattress was and how fluffy the pillows were—very comfortable. The amenities were spot on, the coffeemaker, the gas stove, plenty of dishes and quality cookware to use for making a meal. Not mention, the exceptional decor of the entire apartment! The location was excellent and walkable to numerous local restaurants and bars (a few were literally steps from the apartment door). We loved being on the third floor and always felt super safe. There is some traffic noise that can be heard in the bedroom, but Rob provides earplugs for light sleepers. In truth, we actually enjoyed being lulled to sleep by the sounds of the city and it didn’t bother us at all. The convenience of the grocery store being a 5 minute walk away made buying some pantry and fridge essentials easy. - The couch in the living room was super comfy and perfect for relaxing and watching the Thanksgiving Day parade and some Netflix Christmas movies. This was the perfect Airbnb and we were highly impressed (not always easy). 5 stars to Rob for such an amazingly clean, comfortable, stylish, and well thought out home away from home. We’ll be back soon!\"\r",
      "\r\n",
      "great,\"We enjoyed our stay at Rod's place! The pictures are accurate, and the view is truly amazing. The apartment was clean, and we loved the amenities such as the outdoor pool, the game room with pool table, the gym, and the roof deck where we could see the Navy Pier fireworks in the distance. The location worked well. Not far from the subway, walking distance to many attractions, and several restaurants and convenience stores right there in case we needed something. We did not meet Rod, but he communicated almost immediately when we had questions, offered helpful tips with public transit, and his check-in directions were clear. Very happy with this experience!\"\r",
      "\r\n",
      "great,\"Danielle's place is very roomy, clean, and comfortable to stay in. Everything you need is in her place even a washing machine and dryer. The old stove in the kitchen is a conversation piece in itself. My sister and her family loved staying there. They had two small children and there was plenty of room for everything that they brought. There is also a nice fenced in backyard living space when the weather is nice for you to enjoy. I would highly recommend Danielle's place for your next trip to Chicago. It is within walking distance of Logan Square and all it has to offer.\"\r",
      "\r\n",
      "great,\"If you want to have a a good time in Chicago, don't stay at Danielle's place..... - If you want to have A FABULOUS TIME IN CHICAGO, STAY HERE! - I cannot rave about this place enough. So cozy. So well equipped with everything you could ever need. Danielle has a very charming place with thoughtful touches all around. - Heated floors, a fireplace, fully stocked kitchen and bathroom. We felt right at home. My colleagues and I kept looking around saying, \"\"I love this place. I don't want to leave\"\". On more than one night, we decided to stay in (rather than go out) just so we could enjoy the fire, movie, selection, and some homemade pasta! I will definiltey be recommending this to anyone and everyone who plans to visit Chicago. BONUS: Just down the street from the best diner/vegan food in the world.\"\r",
      "\r\n",
      "great,\"We had a wonderful stay at Heather and JP's apartment. The apartment is very fresh and clean. It's lovely decorated and the kitchenette is well equipped. There is a wonderful garden which is an oasis of peace in the busy city of Chicago. The location is great, only a 5 to 10 minute walk from Division & Milwaukee CTA subway station. Round the corner on Milwaukee Avenue there's a bus stop with buses going downtown. It's very quiet at night which offers you a great sleep. Heather and JP are great hosts who will help you with all your questions. - - All in all we had a very comfortable stay. We would definitely recommend this place to family and friends.\"\r",
      "\r\n",
      "great,\"We absolutely loved our say in Christopher's place. While it is a basement space, it has all the privacy you need including your own entrance. The decor is fantastic and we loved many of the little touches including the board games, tasteful art, game systems, and the water and drinks in the fridge were a welcome surprise after our evening arrival. Having central heat as well as a space heater in the unit helped make sure we stayed nice and toasty during our visit. We didn't take advantage of them, but the fact that a washer and dryer were available was a nice bonus. - - We'll definitely look here first on our next trip to Chicago!\"\r",
      "\r\n",
      "great,\"Scott and Jill are wonderful hosts who worked extremely responsively with us as our scheduled arrival changed. They helped us figure out some of the neighborhoods we were looking for an apartment, gave us some tips on cool places to eat, and introduced us to their sweet doggo, Bonnie. - - Their garage loft is a lovely, very clean space in a very quiet neighborhood. You walk through their beautiful garden and backyard, through their garage, and up into the spacious, well-lit space. There’s a full sized bathroom (bigger than most hotel rooms), decked out kitchen, and comfy sitting nook. - - Great for couples and families, or just the solo traveler that needs some quiet space!\"\r",
      "\r\n",
      "great,\"I took a Chicago staycation to finish my book and Natalia's second-floor apartment was the perfect getaway. It was more than roomy for me, being just one traveler, but I loved the amount of space and light she creates in your classic Chicago \"\"long-but-not-wide\"\" floorplan. There was more than enough light and her cheerful plants and art made me feel right at home. Small touches like a welcome note, snacks, and a guide to the neighborhood were wonderful and I actually found two amazing eateries during my stay thanks to her helpful guidance. - - Water was always hot. Automatic lights and Nest heat made me feel spoiled. And there were ample towels, toiletries, and plateware to be found. I literally rolled in with my suitcase and I was home. - - If I were to make any change, it would be the small addition of having more blankets or comforters available. The heat in the apartment was perfect but I like to sleep under many layers at night (I struggle with this at hotels too!).\"\r",
      "\r\n",
      "great,\"Our stay at Natalia’s was wonderful and we’d come back again - it was as pictured but even better in person. We loved all the plants, the way it was furnished, a very unique space and very comfortable and clean. We enjoyed the coffee, the automatic lighting and smart heating, and little touches like the book with suggestions and handwritten welcome. The area is great too - there’s lots of restaurants and cute shops nearby. Uncharted books was a favourite of ours. In our opinion it was nice to stay out of downtown yet still close to anything you may need. Natalia was also a great host she communicated clearly and even checked in with us to make sure all was well. Thanks Natalia!\"\r",
      "\r\n",
      "great,\"Loveeeeee this place! Its make me and my friend feel like a home-y! We barely couldn’t get out of bed. We had relaxing vibe around here. This is the best best best birthday surprise for my best friend and she love it! So is Marilee, a dog was too chilling! This is our first best experience ever to book the first Airbnb spot! I’m so glad I chose the right one! Thank you so much for allowing us to stay extra night!! We couldn’t stop talking about how much we had so much fun and we literally chilling lot there. Your place are so relaxing and perfect for stress-free mini vacation! - - For anyone you who planning on going to Chicago! Book this place! Trust me, it’s a lifesaver! These hosts are extremely outgoing, lovely couple with sweet personality! They would check on you to make sure everything you need to keep you comfort! - - Oh on behalf, I couldn’t live without their outdoor hot tub and mediated room! ❤️\"\r",
      "\r\n",
      "great,\"Where do I begin?! My boyfriend and I are an eclectic couple, and staying in the artsy, eclectic unit of Steven and Sara’s couldn’t have been more perfect. We loved looking at all the artwork and woodwork in the house, and we were more than comfortable. It honestly felt like staying with friends as Steven and Sara were both very friendly and inviting, and we had a great time conversing with them. They both went out of their way to help us and make us comfortable. We will be back next time we are in Chicago!\""
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Review predictions (first 20):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"0\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8848}, {\"Name\": \"great\", \"Score\": 0.1152}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"1\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8371}, {\"Name\": \"great\", \"Score\": 0.1629}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"2\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9114}, {\"Name\": \"great\", \"Score\": 0.0886}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"3\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.897}, {\"Name\": \"great\", \"Score\": 0.103}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"4\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8981}, {\"Name\": \"great\", \"Score\": 0.1019}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"5\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8554}, {\"Name\": \"great\", \"Score\": 0.1446}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"6\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8696}, {\"Name\": \"great\", \"Score\": 0.1304}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"7\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9113}, {\"Name\": \"great\", \"Score\": 0.0887}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"8\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8951}, {\"Name\": \"great\", \"Score\": 0.1049}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"9\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8921}, {\"Name\": \"great\", \"Score\": 0.108}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"10\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8674}, {\"Name\": \"great\", \"Score\": 0.1326}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"11\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9229}, {\"Name\": \"great\", \"Score\": 0.0771}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"12\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8328}, {\"Name\": \"great\", \"Score\": 0.1672}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"13\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9061}, {\"Name\": \"great\", \"Score\": 0.0939}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"14\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8727}, {\"Name\": \"great\", \"Score\": 0.1274}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"15\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8169}, {\"Name\": \"great\", \"Score\": 0.1831}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"16\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8693}, {\"Name\": \"great\", \"Score\": 0.1307}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"17\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.9178}, {\"Name\": \"great\", \"Score\": 0.0822}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"18\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8178}, {\"Name\": \"great\", \"Score\": 0.1823}]}\r\n",
      "{\"File\": \"airbnb-reviews-holdout.csv\", \"Line\": \"19\", \"Classes\": [{\"Name\": \"notgreat\", \"Score\": 0.8882}, {\"Name\": \"great\", \"Score\": 0.1118}]}\r\n"
     ]
    }
   ],
   "source": [
    "output_s3 = describe_response['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri']\n",
    "#print(output_s3)\n",
    "\n",
    "printmd(\"**Download model from S3 to local filesystem:**\")\n",
    "!aws s3 cp {output_s3} comprehend/output.tar.gz\n",
    "\n",
    "printmd(\"**Uncompress predictions:**\")\n",
    "!tar -xvzf comprehend/output.tar.gz -C comprehend\n",
    "\n",
    "printmd(\"**Review groundtruth (first 20):**\")\n",
    "!head comprehend/airbnb-reviews-holdout-groundtruth.csv -n 5\n",
    "\n",
    "printmd(\"**Review predictions (first 20):**\")\n",
    "!head comprehend/predictions.jsonl -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2 -- Sagemaker model training & delivery using a native algorithm (BlazingText)\n",
    "\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of SageMaker BlazingText to perform supervised binary/multi class with single or multi label text classification. BlazingText can train the model on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. BlazingText extends the fastText text classifier to leverage GPU acceleration using custom CUDA kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "#pod = '9998'\n",
    "#region = 'us-east-1'\n",
    "prefix = 'blazingtext' #we will store the dataset we will use to train a custom classification classifier\n",
    "\n",
    "bucket = \"rga-aws-ai-workshop-pod-\" + pod # Replace with your own bucket name if needed\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Now we'll download a dataset from the web on which we want to train the text classification model. BlazingText expects a single preprocessed text file with space separated tokens and each line of the file should contain a single sentence and the corresponding label(s) prefixed by \"\\__label\\__\".\n",
    "\n",
    "In this example, let us train the text classification model on the [DBPedia Ontology Dataset](https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014#2) as done by [Zhang et al](https://arxiv.org/pdf/1509.01626.pdf). The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014. It has 560,000 training samples and 70,000 testing samples. The fields we used for this dataset contain title and abstract of each Wikipedia article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_file = \"classes.txt\"\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "!ls {prefix} -la\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the dataset and the classes to get some understanding about how the data and the label is provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {prefix}/train.csv -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above output, the CSV has 3 fields - Label index, title and abstract. Let us first create a label index to label name mapping and then proceed to preprocess the dataset for ingestion by BlazingText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will print the labels file (`classes.txt`) to see all possible labels followed by creating an index to label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {prefix}/classes.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the mapping from integer indices to class label which will later be used to retrieve the actual class name during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_to_label = {} \n",
    "#with open(\"dbpedia_csv/classes.txt\") as f:\n",
    "#    for i,label in enumerate(f.readlines()):\n",
    "#        index_to_label[str(i+1)] = label.strip()\n",
    "#print(index_to_label)\n",
    "\n",
    "index_to_label = {} \n",
    "with open(\"blazingtext/classes.txt\", mode='r', encoding='utf-8-sig') as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i)] = label.strip()\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to preprocess the training data into **space separated tokenized text** format which can be consumed by `BlazingText` algorithm. Also, as mentioned previously, the class label(s) should be prefixed with `__label__` and it should be present in the same line along with the original sentence. We'll use `nltk` library to tokenize the input sentences from DBPedia dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the nltk tokenizer and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, mode='r', encoding='utf-8-sig') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#open('blazingtext/train.csv', mode='r', encoding='utf-8-sig')\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess('blazingtext/train.csv', 'blazingtext/airbnb.train', keep=1)\n",
    "\n",
    "# Preparing the validation dataset        \n",
    "preprocess('blazingtext/test.csv', 'blazingtext/airbnb.validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='blazingtext/airbnb.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='blazingtext/airbnb.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for supervised text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the original implementation of [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf), SageMaker BlazingText provides an efficient implementation of the continuous bag-of-words (CBOW) and skip-gram architectures using Negative Sampling, on CPUs and additionally on GPU[s]. The GPU implementation uses highly optimized CUDA kernels. To learn more, please refer to [*BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs*](https://dl.acm.org/citation.cfm?doid=3146347.3146354).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides skip-gram and CBOW, SageMaker BlazingText also supports the \"Batch Skipgram\" mode, which uses efficient mini-batching and matrix-matrix operations ([BLAS Level 3 routines](https://software.intel.com/en-us/mkl-developer-reference-fortran-blas-level-3-routines)). This mode enables distributed word2vec training across multiple CPU nodes, allowing almost linear scale up of word2vec computation to process hundreds of millions of words per second. Please refer to [*Parallelizing Word2Vec in Shared and Distributed Memory*](https://arxiv.org/pdf/1604.04661.pdf) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingText also supports a *supervised* mode for text classification. It extends the FastText text classifier to leverage GPU acceleration using custom CUDA kernels. The model can be trained on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. For more information, please refer to the [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the following modes are supported by BlazingText on different types instances:\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|                \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|      \t|          \t|        ✔       \t|     | |\n",
    "\n",
    "Now, let's define the SageMaker `Estimator` with resource configurations and hyperparameters to train Text Classification on *DBPedia* dataset, using \"supervised\" mode on a `c4.4xlarge` instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.m5.large',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyper-parameters for this object and we have our data channels linked with the algorithm. The only  remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the `Estimator` classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take some time, depending on the size of the data. Therefore it might be a few minutes before we start getting training logs for our training jobs. The data logs will also print out Accuracy on the validation data for every epoch after training job has executed `min_epochs`. This metric is a proxy for the quality of the algorithm. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON format for inference\n",
    "BlazingText supports `application/json` as the content-type for inference. The payload should contain a list of sentences with the key as \"**instances**\" while being passed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head test.csv -n 3\n",
    "\n",
    "all_rows = []\n",
    "with open('blazingtext/test.csv', mode='r', encoding='utf-8-sig') as csvinfile:\n",
    "    csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        all_rows.append(row)\n",
    "#all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "#print(all_rows)\n",
    "#pool = Pool(processes=multiprocessing.cpu_count())\n",
    "#transformed_rows = pool.map(transform_instance, all_rows)\n",
    "#pool.close() \n",
    "#pool.join()\n",
    "\n",
    "#print(all_rows[0][0])\n",
    "#print(all_rows[0][2])\n",
    "\n",
    "\n",
    "\n",
    "#sentences = [\"Convair was an american aircraft manufacturing company which later expanded into rockets and spacecraft.\",\n",
    "#            \"Berwick secondary college is situated in the outer melbourne metropolitan suburb of berwick .\"]\n",
    "\n",
    "# The test data consists of 30 sentences (0-29), select a sentence to validate the prediction\n",
    "sentence = 29\n",
    "\n",
    "sentences = [all_rows[sentence][2]]\n",
    "\n",
    "printmd(\"**Raw Sentence:**\")\n",
    "print(sentences)\n",
    "\n",
    "printmd(\"**Groundtruth (0=Not Great, 1=Great):**\")\n",
    "print(all_rows[sentence][0])\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "\n",
    "printmd(\"**Prediction:**\")\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the model will return only one prediction, the one with the highest probability. For retrieving the top k predictions, you can set `k` in the configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"instances\" : tokenized_sentences,\n",
    "          \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop / Close the Endpoint (Optional)\n",
    "Finally, we should delete the endpoint before we close the notebook if we don't need to keep the endpoint running for serving realtime predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 -- BERT Custom Model Lab\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of SageMaker BlazingText to perform supervised binary/multi class with single or multi label text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY CUSTOMER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "Lets remove the resources we've used during this lab..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE SAGEMAKER ENDPOINT\n",
    "sess.delete_endpoint(text_classifier.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchEntityException",
     "evalue": "An error occurred (NoSuchEntity) when calling the DetachRolePolicy operation: The role with name rga-aws-ai-workshop-pod-9996-role cannot be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchEntityException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ff5400074cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m response = client.detach_role_policy(\n\u001b[1;32m     14\u001b[0m     \u001b[0mRoleName\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-role'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mPolicyArn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpolicyArn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m response = client.detach_role_policy(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchEntityException\u001b[0m: An error occurred (NoSuchEntity) when calling the DetachRolePolicy operation: The role with name rga-aws-ai-workshop-pod-9996-role cannot be found."
     ]
    }
   ],
   "source": [
    "# DELETE REMAINING RESOURCES\n",
    "# S3 Bucket\n",
    "!aws s3 rb s3://{bucket} --force\n",
    "\n",
    "# Comprehend Classifier\n",
    "client = boto3.client('comprehend')\n",
    "response = client.delete_document_classifier(\n",
    "    DocumentClassifierArn=docclassifierarn\n",
    ")\n",
    "\n",
    "# REMOVE IAM ROLE\n",
    "client = boto3.client('iam')\n",
    "response = client.detach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= policyArn\n",
    ")\n",
    "response = client.detach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= 'arn:aws:iam::aws:policy/service-role/ComprehendDataAccessRolePolicy'\n",
    ")\n",
    "response = client.delete_role(\n",
    "    RoleName= bucket + '-role'\n",
    ")\n",
    "\n",
    "# REMOVE IAM POLICY\n",
    "response = client.delete_policy(\n",
    "    PolicyArn= policyArn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
