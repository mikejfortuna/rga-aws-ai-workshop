{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will illustrate the different ways you can consume AI/ML services in AWS with this notebook. We see the Machine Learning stack having three key layers: **Frameworks and Interfaces** for machine learning practitioners, **Platform Services** that make it easy for any developer to get started and get deep with ML, and **Application Services** that enable developers to plug-in pre-built AI functionality into their apps without having to worry about the machine learning models that power these services.\n",
    "\n",
    "\n",
    "\n",
    "## Lab 1 AWS Application Services Layer - Amazon Comprehend\n",
    "\n",
    "**Application services** serve developers and companies who want to add solution-oriented intelligence to their applications through an API call rather than developing and training their own models. For example, in computer vision, we developed Amazon Rekognition to allow developers to easily build intelligent video and image analysis into their applications. [C-SPAN](https://aws.amazon.com/solutions/case-studies/cspan/) is using Amazon Rekognition to automate footage tagging, and prior to Rekognition, they could only index about half of their footage due to the immense amount of manual processes. But, they were able to build a solution using Rekognition in 3 weeks, and can now scan all their footage to identify and retrieve 99,000 political figures, saving approximately 9,000 hours a year in labor.\n",
    "\n",
    "**Lab 1** will use **Amazon Comprehend** a service that uses machine learning to find insights and relationships in text. Amazon Comprehend can identify the language of the text; extract key phrases, places, people, brands, or events; understand how positive or negative the text is; and automatically organize a collection of text files by topic.\n",
    "\n",
    "We will show how to extract sentiment, entities, and key phases using the native service with no ML training required. Next, we will show how to train a Comprehend [Custom Classification model](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html) using the R/GA AirBnb dataset of 1,300 reviews which are labeled great or notgreat to indicate the really amazing 5-star reviews (great) vs the other 5-star reviews not as good. We will use this custom model to score additional reviews.\n",
    "\n",
    "## Lab 2 AWS Platform Services Layer - Amazon Sagemaker\n",
    "\n",
    "The Platform Layer provides an end-to-end platform to coordinate the ML workflow in a seamless and easy way using Amazon Sagemaker. SageMaker removes the complexity that holds back developer success with the many steps in the ML workflow such as: data collection & preparation, algorithm & framework selection, model training & tuning, and model deployment. SageMaker includes modules that can be used together or independently to build, train, and deploy your machine learning models. SageMaker makes it easy to build ML models and get them ready for training by providing everything you need to quickly connect to your training data, and to select and optimize the best algorithm and framework for your application. Amazon SageMaker includes hosted Jupyter notebooks that make it is easy to explore and visualize your training data stored in Amazon S3. You can connect directly to data in S3, or use AWS Glue to move data from Amazon RDS, Amazon DynamoDB, and Amazon Redshift into S3 for analysis in your notebook. SageMaker [includes several common machine learning algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) which have been pre-installed and optimized to deliver up to 10 times the performance you’ll find running these algorithms anywhere else.\n",
    "\n",
    "**Lab 2** will use Sagemaker and a built-in algorithm for natural language processing (NLP) called BlazingText to train a model with the same Airbnb dataset. Once trained we will host a model on a high-level available endpoint hosted in Sagemaker, and we will call that endpoint to provide inference for additional reviews.\n",
    "\n",
    "## AWS Frameworks and Interfaces\n",
    "The Frameworks and interfaces layer is for the expert machine learning practitioners. These are people comfortable building deep learning models, working with deep learning frameworks, building clusters, etc. They can get extremely deep. For this group, the most of machine learning and deep learning being done on the cloud today are being done on AWS with our GPU instances - which are optimized for ML and DL. In 2017, we launched the P3 instance, which is fourteen times faster than the P2 instance. P3 provides a huge boost to the speed and efficiency of deep learning and machine learning workloads.\n",
    "\n",
    "AWS provides choice with regards to frameworks we support and offer (ie Tensorflow, Caffe2, MXNet, Keras, CNTK), we strive to support them all. We will always provide a great solution for all the frameworks and choices that people want to make.\n",
    "\n",
    "**Lab 3** will demonstrate using a pre-existing model [RoBERTa](https://github.com/pytorch/fairseq/tree/master/examples/roberta) with the same AirBnb dataset hosted in a Jupyter notebook within our Sagemaker notebook. We will run the model directly from our Sagemaker notebook, if desired you can [host custom models in Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1 -- Comprehend Lab\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc.\n",
    "\n",
    "Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. Amazon Comprehend processes any text file in UTF-8 format. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases.\n",
    "\n",
    "You work with one or more documents at a time to evaluate their content and gain insights about them. Some of the insights that Amazon Comprehend develops about a document include:\n",
    "\n",
    "- **Entities** – Amazon Comprehend returns a list of entities, such as people, places, and locations, identified in a document. For more information, see Detect Entities.\n",
    "\n",
    "- **Key phrases** – Amazon Comprehend extracts key phrases that appear in a document. For example, a document about a basketball game might return the names of the teams, the name of the venue, and the final score. For more information, see Locate Key Phrases.\n",
    "\n",
    "- **Language** – Amazon Comprehend identifies the dominant language in a document. Amazon Comprehend can identify 100 languages. For more information, see Detect the Dominant Language.\n",
    "\n",
    "- **Sentiment** – Amazon Comprehend determines the emotional sentiment of a document. Sentiment can be positive, neutral, negative, or mixed. For more information, see Determine the Sentiment.\n",
    "\n",
    "- **Syntax** – Amazon Comprehend parses each word in your document and determines the part of speech for the word. For example, in the sentence \"It is raining today in Seattle,\" \"it\" is identified as a pronoun, \"raining\" is identified as a verb, and \"Seattle\" is identified as a proper noun. For more information, see Analyze Syntax.\n",
    "    \n",
    "More information about Amazon Comprehend is [here](https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DEPENDENCIES\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare for the lab by learning the current IAM Role and the S3 bucket we will use...We will need to update the\n",
    "#IAM role. Please review the README.md of the Git repo for me details.\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "# REPLACE THE POD BELOW WITH THE NUMBER ASSIGN TO YOU \n",
    "pod = '9998'\n",
    "\n",
    "# REPLACE THE REGION BELOW WITH THE REGION YOU ARE OPERATING IN (ie N. Virginia=us-east-1, Ohio=us-east-2)\n",
    "region = 'us-east-1'\n",
    "\n",
    "prefix = 'comprehend' #we will store the dataset we will use to train a custom classification classifier\n",
    "\n",
    "bucket = \"rga-aws-ai-workshop-pod-\" + pod # Replace with your own bucket name if needed\n",
    "print(\"The S3 bucket you will be using to upload your dataset is:\", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions to Update the IAM Role for the Sagemaker Notebook\n",
    "Please follow the \n",
    "Need to provide access to:\n",
    "\n",
    "- The S3 bucket we will create\n",
    "- Comprehend\n",
    "- Sagemaker stuff\n",
    "- Create a new IAM role for Comprehend DataAccessRoleArn & include the S3 bucket that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the first Airbnb review through Comprehend native service.\n",
    "client = boto3.client('comprehend')\n",
    "\n",
    "#Lets first run the first Airbnb review through the native sentiment analysis of Comprehend to see whats available\n",
    "#without customization. Please note this is a simple API call with no ML training required.\n",
    "#Feel free to re-run this cell with another review you would like to extract information on.\n",
    "textreview = 'We were In Chicago to see Hamilton and sightsee with our teenaged daughter. We loved this location and neighborhood. It felt safe and there were tons of nearby places to eat or get groceries. The condo itself was extremely clean and was well stocked with necessities. It was spacious and we never felt crowded. Rob was a great host and was very responsive to our questions. It was a plus to have parking available as we never moved our car until we left Chicago. I would definitely rent from Rob again.'\n",
    "printmd(\"**Raw text Airbnb review:**\")\n",
    "print(textreview + '\\n')\n",
    "\n",
    "response = client.detect_sentiment(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "printmd(\"**Comprehend native sentiment analysis:**\")\n",
    "print(json.dumps(response, indent=4))\n",
    "\n",
    "#Note the ability to extract the overall sentiment from the reviews. Comprehend can provide: positive, negative,\n",
    "#neutral, and mixed from the text with no training/tuning\n",
    "#This can also be run from the Comprehend AWS Console interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next lets use Comprehend to detect entities and key phrases for our Airbnb review...\n",
    "response = client.detect_entities(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "printmd(\"**Raw text Airbnb review:**\")\n",
    "print(textreview + '\\n')\n",
    "\n",
    "printmd(\"**Comprehend entities detected:**\")\n",
    "print(json.dumps(response, indent=4) + '\\n')\n",
    "\n",
    "response = client.detect_key_phrases(\n",
    "    Text=textreview,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "printmd(\"**Raw text Airbnb review:**\")\n",
    "print(textreview + '\\n')\n",
    "\n",
    "printmd(\"**Comprehend key phrases detected:**\")\n",
    "print(json.dumps(response, indent=4))\n",
    "\n",
    "#Make note of the different information that can be extracted from the review with no ML model training or tuning....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE BUCKET IN YOUR AWS ACCT to be used for the dataset\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if (region is None) or (region == 'us-east-1'):\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return print(bucket + ' S3 bucket created successfully!')\n",
    "\n",
    "create_bucket(bucket, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the dataset. We are reviewing only the first two entries.\n",
    "printmd(\"**Comprehend working directory contents:**\")\n",
    "!ls comprehend/ -la\n",
    "!pwd\n",
    "\n",
    "printmd(\"**First 2 entries from the training dataset:**\")\n",
    "!head comprehend/airbnb-reviews-training.csv -n 2\n",
    "\n",
    "# We are assuming that the data transformations have occurred upstream and we have formated the data required for\n",
    "# Comprehend in the dataset included in the repo. Note that the data has two columns (comma separated), \n",
    "# the first is a custom label for 'great' or 'notgreat' to indicate the custom labels that have been applied.\n",
    "# This is the format required for the Comprehend custom classifier. The first column is the custom label,\n",
    "# the second column contains the raw review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPLOAD THE DATASET TO S3 FROM THE LOCAL SYSTEM FOR COMPREHEND CUSTOM TRAINING\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return print(file_name + ' object uploaded successfully!')\n",
    "\n",
    "upload_file('comprehend/airbnb-reviews-training.csv', bucket)\n",
    "upload_file('comprehend/airbnb-reviews-holdout.csv', bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create IAM Role for Comprehend to read S3 data from your bucket\n",
    "\n",
    "Comprehend requires permissions to access the dataset in S3. In order to do this we will create an IAM role (Data Access Role) with the required policies to allow Comprehend to assume this role. This role has rights to read from the S3 bucket we created earlier as well as the following native Comprehend policy: _ComprehendDataAccessRolePolicy_.\n",
    "\n",
    "Please see the Comprehend docs for more detail [here](https://docs.aws.amazon.com/comprehend/latest/dg/access-control-managing-permissions.html#auth-role-permissions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE IAM POLICY & ROLE FOR COMPREHEND TO USE TO TRAIN THE CUSTOM MODEL\n",
    "# CREATE IAM POLICY\n",
    "policyDocumentStr = '''\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"s3:GetObject\",\n",
    "            \"s3:ListBucket\",\n",
    "            \"s3:PutObject\"\n",
    "        ],\n",
    "        \"Resource\": [\n",
    "            \"arn:aws:s3:::%s\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "'''%(bucket)\n",
    "pattern = re.compile(r'[\\s\\r\\n]+')\n",
    "policyDocumentStr = re.sub(pattern, '', policyDocumentStr)\n",
    "\n",
    "client = boto3.client('iam')\n",
    "response = client.create_policy(\n",
    "    PolicyName= bucket + '-policy',\n",
    "    PolicyDocument=policyDocumentStr,\n",
    "    Description='IAM permissions policy for Comprehend access to S3 bucket for RGA AWS AI Workshop'\n",
    ")\n",
    "print('The ' + response['Policy']['PolicyName'] + ' IAM policy was created')\n",
    "policyArn= response['Policy']['Arn']\n",
    "\n",
    "# CREATE IAM ROLE\n",
    "trustPolicyDocumentStr = '''\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"comprehend.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}'''\n",
    "pattern = re.compile(r'[\\s\\r\\n]+')\n",
    "trustPolicyDocumentStr = re.sub(pattern, '', trustPolicyDocumentStr)\n",
    "\n",
    "response = client.create_role(\n",
    "    RoleName= bucket + '-role',\n",
    "    AssumeRolePolicyDocument= trustPolicyDocumentStr,\n",
    "    Description='IAM role for Comprehend access to S3 bucket for RGA AWS AI Workshop'\n",
    ")\n",
    "print('The ' + response['Role']['RoleName'] + ' IAM role was created')\n",
    "dataaccessarn=response['Role']['Arn']\n",
    "\n",
    "# ASSIGN POLICIES TO ROLE\n",
    "response = client.attach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= policyArn\n",
    ")\n",
    "response = client.attach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= 'arn:aws:iam::aws:policy/service-role/ComprehendDataAccessRolePolicy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Comprehend Custom Classifier\n",
    "You can customize Comprehend for your specific requirements without the skillset required to build machine learning-based NLP solutions. Using automatic machine learning, or AutoML, Comprehend Custom builds customized NLP models on your behalf, using data you already have.\n",
    "\n",
    "Amazon Comprehend uses a proprietary, state-of-the-art sequence tagging deep neural network model that powers the same Amazon Comprehend detect entities service to train your custom entity recognizer models. In addition, we understand that acquiring training data could be costly. To help customers build a highly accurate model with limited amount of data, Amazon Comprehend uses a technique called transfer learning to train your custom models based on an sophisticated general-purpose entities recognition model that was pre-trained with a large amount of data we collected from multiple domains. Offline experiments showed that transfer learning significantly improved custom entity recognizer model accuracy especially when the amount of training data is small.\n",
    "\n",
    "To learn more about training a Comprehend Custom Classifier please review the docs [here](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification-training.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE COMPREHEND CUSTOM CLASSIFIER\n",
    "client = boto3.client('comprehend', region_name=region)\n",
    "s3_uri = 's3://' + bucket + '/' + prefix + '/' + 'airbnb-reviews-training.csv'\n",
    "\n",
    "#UPDATE THE ARN FROM THE ROLE YOU CREATED IN THE PREVIOUS SECTION\n",
    "#account_id = role.split(':')[4]\n",
    "#dataaccessarn='arn:aws:iam::12345678901234:role/rga-aws-ai-workshop-pod-xxxx'\n",
    "                    \n",
    "docclassifier='RGAAirbnb-' + pod\n",
    "\n",
    "# Create a document classifier\n",
    "create_response = client.create_document_classifier(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_uri\n",
    "    },\n",
    "    DataAccessRoleArn=dataaccessarn,\n",
    "    DocumentClassifierName=docclassifier,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "printmd(\"**Create response output**\")\n",
    "print(str(create_response) + \"\\n\")\n",
    "\n",
    "# Check the status of the classifier\n",
    "docclassifierarn = create_response['DocumentClassifierArn']\n",
    "describe_response = client.describe_document_classifier(\n",
    "    DocumentClassifierArn=docclassifierarn)\n",
    "printmd(\"**Describe response output:**\")\n",
    "print(str(describe_response))\n",
    "\n",
    "# Review the output below, we have requested that the Comprehend customer classifier be created. It should take \n",
    "# Several minutes to train the model using the AirBnb dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the classifier\n",
    "# Look for a status on TRAINED before moving on to the next step\n",
    "describe_response = client.describe_document_classifier(DocumentClassifierArn=docclassifierarn)\n",
    "printmd(\"**Document classifier status (Wait for the status to show TRAINED):**\")\n",
    "print(describe_response['DocumentClassifierProperties']['Status'])\n",
    "while describe_response['DocumentClassifierProperties']['Status'] != 'TRAINED':\n",
    "    time.sleep(20)\n",
    "    describe_response = client.describe_document_classifier(DocumentClassifierArn=docclassifierarn)\n",
    "    print(describe_response['DocumentClassifierProperties']['Status'])\n",
    "\n",
    "printmd(\"**Custom Model Accuracy:**\")\n",
    "print(str(describe_response['DocumentClassifierProperties']['ClassifierMetadata']['EvaluationMetrics']))\n",
    "\n",
    "# Alternatively please review the Comprehend AWS Console GUI to validate if your custom job is still training\n",
    "# Please note the output below, the custom model accuracy is tested and provided in the output of the model after\n",
    "# training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('comprehend', region_name=region)\n",
    "s3_uri_in = 's3://' + bucket + '/' + prefix + '/' + 'airbnb-reviews-holdout.csv'\n",
    "s3_uri_out = 's3://' + bucket + '/' + prefix + '/' + 'output'\n",
    "jobname='RGAAirbnb-Job-' + pod\n",
    "\n",
    "start_response = client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_uri_in,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_uri_out\n",
    "    },\n",
    "    DataAccessRoleArn=dataaccessarn,\n",
    "    DocumentClassifierArn=docclassifierarn,\n",
    "    JobName=jobname\n",
    ")\n",
    "\n",
    "printmd(\"**Start response output:**\")\n",
    "print(str(start_response) + '\\n')\n",
    "\n",
    "# Check the status of the job\n",
    "describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "printmd(\"**Describe response output:**\")\n",
    "print(describe_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the job.\n",
    "# When JobStatus is COMPLETED you can move to the next step.\n",
    "describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "\n",
    "printmd(\"**Job training status (Wait for the status to show COMPLETED):**\")\n",
    "print(describe_response['DocumentClassificationJobProperties']['JobStatus'])\n",
    "\n",
    "while describe_response['DocumentClassificationJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    time.sleep(20)\n",
    "    describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "    print(describe_response['DocumentClassificationJobProperties']['JobStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_s3 = describe_response['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri']\n",
    "#print(output_s3)\n",
    "\n",
    "printmd(\"**Download model from S3 to local filesystem:**\")\n",
    "!aws s3 cp {output_s3} comprehend/output.tar.gz\n",
    "\n",
    "printmd(\"**Uncompress predictions:**\")\n",
    "!tar -xvzf comprehend/output.tar.gz -C comprehend\n",
    "\n",
    "printmd(\"**Review groundtruth (first 5):**\")\n",
    "!head comprehend/airbnb-reviews-holdout-groundtruth.csv -n 5\n",
    "\n",
    "printmd(\"**Review predictions (first 5):**\")\n",
    "!head comprehend/predictions.jsonl -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review \n",
    "\n",
    "predict=1\n",
    "\n",
    "printmd(\"**Review groundtruth:**\")\n",
    "!sed '{predict}!d' comprehend/airbnb-reviews-holdout-groundtruth.csv\n",
    "\n",
    "printmd(\"**Review predictions:**\")\n",
    "!sed '{predict}!d' comprehend/predictions.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2 -- Sagemaker model training & delivery using a native algorithm (BlazingText)\n",
    "\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of SageMaker BlazingText to perform supervised binary/multi class with single or multi label text classification. BlazingText can train the model on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. BlazingText extends the fastText text classifier to leverage GPU acceleration using custom CUDA kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "#pod = '9998'\n",
    "#region = 'us-east-1'\n",
    "prefix = 'blazingtext' #we will store the dataset we will use to train a custom classification classifier\n",
    "\n",
    "bucket = \"rga-aws-ai-workshop-pod-\" + pod # Replace with your own bucket name if needed\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Now we'll download a dataset from the web on which we want to train the text classification model. BlazingText expects a single preprocessed text file with space separated tokens and each line of the file should contain a single sentence and the corresponding label(s) prefixed by \"\\__label\\__\".\n",
    "\n",
    "In this example, let us train the text classification model on the [DBPedia Ontology Dataset](https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014#2) as done by [Zhang et al](https://arxiv.org/pdf/1509.01626.pdf). The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014. It has 560,000 training samples and 70,000 testing samples. The fields we used for this dataset contain title and abstract of each Wikipedia article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_file = \"classes.txt\"\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "!ls {prefix} -la\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the dataset and the classes to get some understanding about how the data and the label is provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {prefix}/train.csv -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above output, the CSV has 3 fields - Label index, title and abstract. Let us first create a label index to label name mapping and then proceed to preprocess the dataset for ingestion by BlazingText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will print the labels file (`classes.txt`) to see all possible labels followed by creating an index to label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {prefix}/classes.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the mapping from integer indices to class label which will later be used to retrieve the actual class name during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_to_label = {} \n",
    "#with open(\"dbpedia_csv/classes.txt\") as f:\n",
    "#    for i,label in enumerate(f.readlines()):\n",
    "#        index_to_label[str(i+1)] = label.strip()\n",
    "#print(index_to_label)\n",
    "\n",
    "index_to_label = {} \n",
    "with open(\"blazingtext/classes.txt\", mode='r', encoding='utf-8-sig') as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i)] = label.strip()\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to preprocess the training data into **space separated tokenized text** format which can be consumed by `BlazingText` algorithm. Also, as mentioned previously, the class label(s) should be prefixed with `__label__` and it should be present in the same line along with the original sentence. We'll use `nltk` library to tokenize the input sentences from our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the nltk tokenizer and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, mode='r', encoding='utf-8-sig') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#open('blazingtext/train.csv', mode='r', encoding='utf-8-sig')\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess('blazingtext/train.csv', 'blazingtext/airbnb.train', keep=1)\n",
    "\n",
    "# Preparing the validation dataset        \n",
    "preprocess('blazingtext/test.csv', 'blazingtext/airbnb.validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='blazingtext/airbnb.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='blazingtext/airbnb.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for supervised text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the original implementation of [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf), SageMaker BlazingText provides an efficient implementation of the continuous bag-of-words (CBOW) and skip-gram architectures using Negative Sampling, on CPUs and additionally on GPU[s]. The GPU implementation uses highly optimized CUDA kernels. To learn more, please refer to [*BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs*](https://dl.acm.org/citation.cfm?doid=3146347.3146354).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides skip-gram and CBOW, SageMaker BlazingText also supports the \"Batch Skipgram\" mode, which uses efficient mini-batching and matrix-matrix operations ([BLAS Level 3 routines](https://software.intel.com/en-us/mkl-developer-reference-fortran-blas-level-3-routines)). This mode enables distributed word2vec training across multiple CPU nodes, allowing almost linear scale up of word2vec computation to process hundreds of millions of words per second. Please refer to [*Parallelizing Word2Vec in Shared and Distributed Memory*](https://arxiv.org/pdf/1604.04661.pdf) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingText also supports a *supervised* mode for text classification. It extends the FastText text classifier to leverage GPU acceleration using custom CUDA kernels. The model can be trained on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. For more information, please refer to the [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the following modes are supported by BlazingText on different types instances:\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|                \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|      \t|          \t|        ✔       \t|     | |\n",
    "\n",
    "Now, let's define the SageMaker `Estimator` with resource configurations and hyperparameters to train Text Classification on *DBPedia* dataset, using \"supervised\" mode on a `m5.large` instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.m5.large',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyper-parameters for this object and we have our data channels linked with the algorithm. The only  remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the `Estimator` classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take some time, depending on the size of the data. Therefore it might be a few minutes before we start getting training logs for our training jobs. The data logs will also print out Accuracy on the validation data for every epoch after training job has executed `min_epochs`. This metric is a proxy for the quality of the algorithm. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)\n",
    "\n",
    "## Review the output below to see the model statistics and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON format for inference\n",
    "BlazingText supports `application/json` as the content-type for inference. The payload should contain a list of sentences with the key as \"**instances**\" while being passed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head test.csv -n 3\n",
    "\n",
    "all_rows = []\n",
    "with open('blazingtext/test.csv', mode='r', encoding='utf-8-sig') as csvinfile:\n",
    "    csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        all_rows.append(row)\n",
    "#all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "#print(all_rows)\n",
    "#pool = Pool(processes=multiprocessing.cpu_count())\n",
    "#transformed_rows = pool.map(transform_instance, all_rows)\n",
    "#pool.close() \n",
    "#pool.join()\n",
    "\n",
    "#print(all_rows[0][0])\n",
    "#print(all_rows[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the model will return only one prediction, the one with the highest probability. For retrieving the top k predictions, you can set `k` in the configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test data consists of 30 Airbnb reviews (0-29), select a sentence to validate the prediction\n",
    "sentence = 10\n",
    "\n",
    "sentences = [all_rows[sentence][2]]\n",
    "\n",
    "printmd(\"**Raw Sentence:**\")\n",
    "print(sentences)\n",
    "\n",
    "printmd(\"**Groundtruth (0=Not Great, 1=Great):**\")\n",
    "print(all_rows[sentence][0])\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "\n",
    "printmd(\"**Prediction:**\")\n",
    "print(json.dumps(predictions, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "Lets remove the resources we've used during this lab...\n",
    "\n",
    "We should delete the Sagemaker endpoint before we close the notebook if we don't need to keep the endpoint running for serving realtime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE SAGEMAKER ENDPOINT\n",
    "sess.delete_endpoint(text_classifier.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE REMAINING RESOURCES\n",
    "# S3 Bucket\n",
    "!aws s3 rb s3://{bucket} --force\n",
    "\n",
    "# Comprehend Classifier\n",
    "client = boto3.client('comprehend')\n",
    "response = client.delete_document_classifier(\n",
    "    DocumentClassifierArn=docclassifierarn\n",
    ")\n",
    "\n",
    "# REMOVE IAM ROLE\n",
    "client = boto3.client('iam')\n",
    "response = client.detach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= policyArn\n",
    ")\n",
    "response = client.detach_role_policy(\n",
    "    RoleName= bucket + '-role',\n",
    "    PolicyArn= 'arn:aws:iam::aws:policy/service-role/ComprehendDataAccessRolePolicy'\n",
    ")\n",
    "response = client.delete_role(\n",
    "    RoleName= bucket + '-role'\n",
    ")\n",
    "\n",
    "# REMOVE IAM POLICY\n",
    "response = client.delete_policy(\n",
    "    PolicyArn= policyArn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
